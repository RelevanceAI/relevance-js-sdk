/**
 * This file was auto-generated by openapi-typescript.
 * Do not make direct changes to the file.
 */

export interface paths {
  "/datasets/{dataset_id}/documents/insert": {
    /**
     * When inserting the document you can specify your own id for a document by using the field name **"\_id"**.
     * For specifying your own vector use the suffix (ends with)  **"\_vector\_"** for the field name.
     * e.g. "product\_description\_vector\_"
     */
    post: operations["Insert"];
  };
  "/datasets/{dataset_id}/documents/bulk_insert": {
    /**
     * When inserting documents you can specify your own id for a document by using the field name **"\_id"**.
     * For specifying your own vector use the suffix (ends with)  **"\_vector\_"** for the field name.
     * e.g. "product\_description\_vector\_"
     */
    post: operations["BulkInsert"];
  };
  "/datasets/{dataset_id}/blob_access_urls": {
    /** Generate an upload and download url for temporary storing of large files. Used in conjunction with parse_blob to upload large datasets. */
    post: operations["BlobAccessURLs"];
  };
  "/datasets/{dataset_id}/parse_blob": {
    /** Bulk insert a large number of documents by downloading a file using "download url". This bypasses the need to directly send documents to the api, as happens in BulkInsert. */
    post: operations["ParseBlob"];
  };
  "/datasets/{dataset_id}/documents/update": {
    /** Edit by providing a key value pair of fields you are adding or changing. */
    post: operations["Update"];
  };
  "/datasets/{dataset_id}/documents/bulk_update": {
    /** Edits documents by providing a key value pair of fields you are adding or changing, make sure to include the "_id" in the documents. */
    post: operations["BulkUpdate"];
  };
  "/datasets/{dataset_id}/documents/update_where": {
    /** Updates documents by filters. The updates to make to the documents that is returned by a filter. The updates should be specified in a format of {"field_name": "value"}. e.g. {"item.status" : "Sold Out"} */
    post: operations["UpdateWhere"];
  };
  "/datasets/{dataset_id}/simple_search": {
    /**
     * All properties supported in the POST body can be set in query parameters when using GET method.
     *     It returns the same response body SimpleSearchPost
     *
     *
     *     For example:
     *
     *     /latest/datasets/:dataset_id/simple_search?query=c&page=1&pageSize=1&fieldsToAggregate=["filter_field"]
     *
     *
     *
     *     Integer and string properies such as query, page, pageSize should be directly included as query parameters.
     *
     *     Object properties such as "fieldsToAggregate" should be stringified as JSON, then included as parameters.
     *
     *
     *
     *     For example, in javascript, use the following code to add an object query parameter:
     *
     *     "/latest/datasets/:dataset_id/simple_search?fieldsToAggregate="+JSON.stringify(["filter_field"])
     */
    get: operations["SimpleSearchGet"];
    /** SimpleSearch is an easy way to use vector search and traditional text matching to search your dataset. It also supports filtering, sorting and aggregating results. */
    post: operations["SimpleSearchPost"];
  };
}

export interface components {
  schemas: {
    InsertInput: {
      /** Whether to include insert date as a field 'insert_date_'. */
      insert_date?: boolean;
      /** Whether to overwrite document if it exists. */
      overwrite?: boolean;
      /** Whether the api should check the documents for vector datatype to update the schema. */
      update_schema?: boolean;
      /** Whether to return before all documents have finished updating. */
      wait_for_update?: boolean;
      /** Before insertion, fields of documents will be transformed according to this array of rules. If transformation fails, the output field will not be generated. */
      field_transformers?: {
        /** Field to transform. to transform a nested field, use a.b */
        field: string;
        /** If specified, place result in this field. To place in a nested field, use format a.b */
        output_field?: string;
        /** Whether to strip html tags from field. */
        remove_html?: boolean;
        /** Whether transform a text field into an array of text by splitting sentences. */
        split_sentences?: boolean;
      }[];
      /** A list of jobs to run once all documents are processed. */
      jobs_to_trigger?: {
        /** Specify this to run an encoding job after documents are processed. Supports chunk field encoding, encoding many fields, and storing results in a new field specified by alias. */
        encode?: {
          /** Chunk field to encode. Will produce a field with _chunkvector_ suffix. */
          chunk_field?: string;
          /** Regular fields to encode. Will produce a field with _vector suffix. */
          fields?: string[];
          /** If provided, this string will be added to the output field name. */
          alias?: string;
          /** Model name to use for encoding. */
          model_name?: string;
        };
      }[];
      /** Each document to upsert must have an _id field matching an existing document. */
      document?: {
        _id?: unknown;
      } & { [key: string]: unknown };
    };
    InsertOutput: unknown;
    BulkInsertInput: {
      /** Whether to include insert date as a field 'insert_date_'. */
      insert_date?: boolean;
      /** Whether to overwrite document if it exists. */
      overwrite?: boolean;
      /** Whether the api should check the documents for vector datatype to update the schema. */
      update_schema?: boolean;
      /** Whether to return before all documents have finished updating. */
      wait_for_update?: boolean;
      /** Before insertion, fields of documents will be transformed according to this array of rules. If transformation fails, the output field will not be generated. */
      field_transformers?: {
        /** Field to transform. to transform a nested field, use a.b */
        field: string;
        /** If specified, place result in this field. To place in a nested field, use format a.b */
        output_field?: string;
        /** Whether to strip html tags from field. */
        remove_html?: boolean;
        /** Whether transform a text field into an array of text by splitting sentences. */
        split_sentences?: boolean;
      }[];
      /** A list of jobs to run once all documents are processed. */
      jobs_to_trigger?: {
        /** Specify this to run an encoding job after documents are processed. Supports chunk field encoding, encoding many fields, and storing results in a new field specified by alias. */
        encode?: {
          /** Chunk field to encode. Will produce a field with _chunkvector_ suffix. */
          chunk_field?: string;
          /** Regular fields to encode. Will produce a field with _vector suffix. */
          fields?: string[];
          /** If provided, this string will be added to the output field name. */
          alias?: string;
          /** Model name to use for encoding. */
          model_name?: string;
        };
      }[];
      documents?: ({
        _id?: unknown;
      } & { [key: string]: unknown })[];
    };
    BulkInsertOutput: {
      /** Number of successfully processed documents. */
      inserted: number;
      /** Information about documents that were not processed successfully. */
      failed_documents: {
        /** _id field of unprocessed document. */
        _id?: unknown;
        error?: {
          /** http status code of individual document insertion operation. */
          status?: number;
          /** body response of individual document insertion operation. */
          body?: string;
        };
      }[];
    };
    BlobAccessURLsInput: unknown;
    BlobAccessURLsOutput: {
      /** Used to upload a blob of documents. */
      upload_blob_url: string;
      /** Used to download a blob of products. Used in the ParseBlob endpoint */
      download_blob_url: string;
    };
    ParseBlobInput: {
      /** Used to download the blob of documents. */
      blob_url: string;
      /** Whether the blob is in csv or json format. */
      format: "csv" | "json";
      /** replace _id with (_id||id)['$oid'], or JSON.stringify(_id||id) */
      process_id?: boolean;
      /** Only process rows of length skip_rows. */
      skip_rows?: number;
    };
    ParseBlobOutput: {
      status: string;
      /** Response message or error message. */
      message: string;
      /** Number of successfully inserted documents. */
      inserted?: number;
      /** Failed documents. */
      failed_documents?: unknown[];
    };
    UpdateInput: {
      /** _id of document to update. */
      id: unknown;
      /** Each document to upsert must have an _id field matching an existing document. */
      updates: {
        _id?: unknown;
      } & { [key: string]: unknown };
      /** Whether to include insert date as a field 'insert_date_'. */
      insert_date?: boolean;
      /** Whether to return before all documents have finished updating. */
      wait_for_update?: boolean;
      /** Before insertion, fields of documents will be transformed according to this array of rules. If transformation fails, the output field will not be generated. */
      field_transformers?: {
        /** Field to transform. to transform a nested field, use a.b */
        field: string;
        /** If specified, place result in this field. To place in a nested field, use format a.b */
        output_field?: string;
        /** Whether to strip html tags from field. */
        remove_html?: boolean;
        /** Whether transform a text field into an array of text by splitting sentences. */
        split_sentences?: boolean;
      }[];
    };
    UpdateOutput: {
      /** Completion status. */
      status: string;
      /** Response message for completion state. */
      message: string;
    };
    BulkUpdateInput: {
      /** List of updates to apply to documents. Each update item must contain an _id. */
      updates: ({
        _id?: unknown;
      } & { [key: string]: unknown })[];
      /** Whether to include insert date as a field 'insert_date_'. */
      insert_date?: boolean;
      /** Whether to return before all documents have finished updating. */
      wait_for_update?: boolean;
      /** Before insertion, fields of documents will be transformed according to this array of rules. If transformation fails, the output field will not be generated. */
      field_transformers?: {
        /** Field to transform. to transform a nested field, use a.b */
        field: string;
        /** If specified, place result in this field. To place in a nested field, use format a.b */
        output_field?: string;
        /** Whether to strip html tags from field. */
        remove_html?: boolean;
        /** Whether transform a text field into an array of text by splitting sentences. */
        split_sentences?: boolean;
      }[];
    };
    BulkUpdateOutput: {
      /** Number of successfully processed documents. */
      inserted: number;
      /** Information about documents that were not processed successfully. */
      failed_documents: {
        /** _id field of unprocessed document. */
        _id?: unknown;
        error?: {
          /** http status code of individual document insertion operation. */
          status?: number;
          /** body response of individual document insertion operation. */
          body?: string;
        };
      }[];
    };
    UpdateWhereInput: {
      /** Updates to make to the documents. It should be specified in a format of {"field_name": "value"}. e.g. {"item.status" : "Sold Out"} */
      updates: { [key: string]: unknown };
      /** Array of objects. Each objects fields must match a documents corresponding field for a match */
      filters?: { [key: string]: unknown }[];
    };
    UpdateWhereOutput: {
      /** Completion status. */
      status: string;
      /** Response message for completion state. */
      message: string;
    };
    /**
     * Control which fields aggregate data will be generated for. Aggregate data will appear in the "aggregations" property of the response body.
     *
     *     Each list element can be a string like "name" or a dictionary like {"key":"name","resultsSize":11}
     *
     *     Default: []
     *
     *     Example: ["name","price","likes_cats",{"key":"color","resultsSize":20}]
     */
    fieldsToAggregate: (
      | string
      | {
          /** Field to aggregate. */
          key: string;
          /** Number top aggregate results to return. */
          resultsSize?: number;
          fieldsToAggregate?: components["schemas"]["fieldsToAggregate"];
        }
    )[];
    vectorSearchQuery: {
      /** Vector name to search on. For example, title_vector_ */
      field: string;
      /** Query to transform to a vector and then search with. Default to query in the root body if not provided. */
      query?: string;
      /** Model name to generate the vector with. */
      model?: string;
      /** Model url to use for encoding. If model and model_url are both set, model_url will override it. */
      model_url?: string;
      /** Increases or decreases the impact of this vector fields match on documents relevance score. */
      weight?: number;
      chunkConfig?: {
        chunkField: string;
        page?: number;
        pageSize?: number;
      };
    };
    filterListItem: {
      /** Match where document[key] is in value list. */
      match?: {
        /** Field to match on. */
        key: string;
        /** List of items that to match on. */
        value: unknown[];
      };
      /** Match documents where greaterThan < document[key] < lessThan. Supports numbers and date strings. */
      range?: {
        /** Field to match on. */
        key: string;
        greaterThan?: unknown;
        lessThan?: unknown;
      };
      wildcard?: {
        /** Field to match on. */
        key: string;
        /** array of valid wildcard strings to match on, for example ['tele*'] */
        value: string[];
      };
      /** Match documents where document[a] <=/>=/</>/==/!= document[b]. */
      selfreference?: {
        /** First field in comparison. */
        a: string;
        /** Second field in comparison. */
        b: string;
        /** Operator used to compare a and b. */
        operation: "<=" | ">=" | "<" | ">" | "==" | "!=";
      };
      /** Used to perform a logical OR of filters. each element of the OR list can itself be a list  to perform a nested AND. {or:[[A,B],C]} is equivalent to (A AND B) OR C */
      or?: (
        | components["schemas"]["filterListItem"][]
        | components["schemas"]["filterListItem"]
      )[];
      /** Used to perform NOT filter. Can be a single filter or a list of filters to perform a !(AND). {not:[A,B]} is equivalent to !(A AND B) */
      not?:
        | components["schemas"]["filterListItem"][]
        | components["schemas"]["filterListItem"];
    };
    relevanceBoosterItem: {
      /** Match where document[key] is in value list. */
      match?: {
        /** Field to match on. */
        key: string;
        /** List of items that to match on. */
        value: unknown[];
      };
      /** Match documents where greaterThan < document[key] < lessThan. Supports numbers and date strings. */
      range?: {
        /** Field to match on. */
        key: string;
        greaterThan?: unknown;
        lessThan?: unknown;
      };
      wildcard?: {
        /** Field to match on. */
        key: string;
        /** array of valid wildcard strings to match on, for example ['tele*'] */
        value: string[];
      };
      /** Match documents where document[a] <=/>=/</>/==/!= document[b]. */
      selfreference?: {
        /** First field in comparison. */
        a: string;
        /** Second field in comparison. */
        b: string;
        /** Operator used to compare a and b. */
        operation: "<=" | ">=" | "<" | ">" | "==" | "!=";
      };
      /** Used to perform a logical OR of filters. each element of the OR list can itself be a list  to perform a nested AND. {or:[[A,B],C]} is equivalent to (A AND B) OR C */
      or?: (
        | components["schemas"]["filterListItem"][]
        | components["schemas"]["filterListItem"]
      )[];
      /** Used to perform NOT filter. Can be a single filter or a list of filters to perform a !(AND). {not:[A,B]} is equivalent to !(A AND B) */
      not?:
        | components["schemas"]["filterListItem"][]
        | components["schemas"]["filterListItem"];
      /** Multiplies the relevance contribution of this condition. */
      weight?: number;
    };
    SimpleSearchPostInput: {
      /**
       * Search for documents that contain this query string in your dataset. Use fieldsToSearch parameter to restrict which fields are searched.
       *
       *     "tele" matches "Television", "This television is an excellent product…"
       *
       *     Example: "tele"
       */
      query?: string;
      /** Configuration for traditional search query. */
      queryConfig?: {
        /** Increases or decreases the impact of traditional search when calculating a documents _relevance. new_traditional_relevance = traditional_relevance*queryConfig.weight */
        weight?: number;
      };
      /**
       * Prioritise the result list of documents based on semantic similarity to "query" provided here.
       *
       *     For example if field "animaltype_vector_" contains encoded vector values for "cat", lion, "dog","bird", and "query" is set to "kitten", documents with "cat", "lion" will be returned first in the results list.
       *
       *     It can be an object or a list of objects.
       *
       *
       *
       *     Example payloads:
       *
       *     {"field":"animaltype_vector_","query":"kitten"}
       *
       *     [
       *
       *     {"field":"animaltype_vector_","query":"kitten","weight":1","model":"text"}, {"field":"animaltype_vector_","query":"https://www.dogimage.com/dogimage.png","model":"image","weight":2}
       *
       *     ]
       */
      vectorSearchQuery?:
        | components["schemas"]["vectorSearchQuery"]
        | components["schemas"]["vectorSearchQuery"][];
      /**
       * A list of fields to search using the "query" parameter.
       *
       *     Each item can be field name as a string, or an object with 'key' as field name and optional parameters such as field weight.
       *
       *     Default behaviour is to search all fields.
       *
       *     Example: ["name",{"key":"favourite_color","weight":0.2}]
       */
      fieldsToSearch?: (
        | string
        | {
            /** Field name to search. */
            key: string;
            /** Multiply the relevance contribution of a specific field when using traditional search. */
            weight?: number;
          }
      )[];
      /**
       * Page of results to return.
       *     Returns results from position page*pageSize to (page+1)*pageSize
       *
       *     Default: 0
       *
       *     Example: 3
       */
      page?: number;
      /**
       * Page size of results to return.
       *     Returns results from position page*pageSize to (page+1)*pageSize
       *
       *     Default: 10
       *
       *     Example: 25
       */
      pageSize?: number;
      /**
       * Only return documents with a _relevance above this threshold.
       *
       *     Example: 0.3
       */
      minimumRelevance?: number;
      /**
       * Use the datasets schema to remove parameters that are unsafe before making a request.
       *
       *     For example, fieldsToAggregateStats only supports numeric fields. if this flag is set, all non numeric fields will be removed from fieldsToAggregateStats.
       *
       *     Example: true
       */
      cleanPayloadUsingSchema?: boolean;
      /**
       * Prioritise results based on integer, float and boolean fields values. Can sort ascending or descending.
       *
       *     Example 1: {"on_sale":"desc"}
       *
       *     Example 2: {"price":"asc"}
       */
      sort?: { [key: string]: "asc" | "desc" };
      /**
       * Only return fields of documents listed in this array.
       *
       *     Example: ["name","description_vector_"]
       */
      includeFields?: string[];
      /**
       * Don't return fields of documents listed in this array.
       *
       *     Example: ["name","description_vector"]
       */
      excludeFields?: string[];
      /**
       * Set to true to return all vector fields. includeFields / excludeFields has priority over this rule.
       *
       *     Example: true / false
       */
      includeVectors?: boolean;
      /**
       * Prioritise results based on integer, float and boolean fields values. Can sort ascending or descending.
       *
       *     Example 1: {"on_sale":"desc"}
       *
       *     Example 2: {"price":"asc"}
       */
      textSort?: { [key: string]: "asc" | "desc" };
      fieldsToAggregate?: components["schemas"]["fieldsToAggregate"];
      /**
       * Control which fields aggregate stats data will be generated for. This includes, min value, max value, average value, and document counts per interval within the dataset.
       *
       *     Aggregate data will appear in the "aggregateStats" property of the response body.
       *
       *     Each list item can be:
       *
       *      a string stating which field to aggregate stats on
       *
       *     An object containing "key" to pick a field, and "interval" to control the range of each bucket when counting documents:
       *
       *     For example, if we had products {"price":50},{"price:"150"},{price:"180"},{"key":"price","interval":100}  would split results into 2 buckets.
       *
       *     Default: []
       *     Example: ["postcode",{"key":"price","interval":10}]
       */
      fieldsToAggregateStats?: (
        | string
        | {
            /** Field to return stats for in aggregateStats. */
            key: string;
            /** Interval gap when building a histogram. An interval of 2 will count documents in range 0-2,2-4,4-6... */
            interval?: number;
          }
      )[];
      /**
       * Only Include results that match on this list of filters.
       *
       *     "match" property of filters requires "key" to control which field to match on, and an array of items in "value" to dictate exact match values.
       *
       *     "range" property of filters requires "key" to control which field to match on, and "greaterThan" and/or "lessThan", which must be a number or a date string.
       *
       *     "wildcard" property of filters requires "key" to control which field to match on, and an array of items in "value" to control wildcard match pattern. It may include * to match any string of characters, and ? To match one character.
       *
       *     "or" property of filters requires either: a list of ( subfilters / list of subfilters) etc. [{"match":...},[{"match":...},{"range":...}]].At least one of the subfilters must evaluate to true. If a subfilter is an array, it performs an AND over the sub-sub-filters. For example, [filter1,[filter2,filter3]] is equivalent to filter1 OR (filter2 AND filter3).
       *
       *     Example:
       *     [
       *
       *       {
       *
       *         "match":{
       *
       *           "key":"name",
       *
       *           "value":["television"]
       *
       *         }
       *
       *       },
       *
       *       {
       *
       *         "wildcard":{
       *
       *           "key":"name",
       *
       *           "value":["television*"]
       *
       *         }
       *
       *       },
       *
       *
       *
       *       {
       *
       *         "range":{
       *
       *           "key":"price",
       *
       *           "greaterThan":5.5,
       *
       *           "lessThan":20
       *
       *         }
       *
       *       },
       *
       *       {
       *
       *         "range":{
       *
       *           "key":"postcode",
       *
       *           "greaterThan":2000
       *
       *         }
       *
       *       },
       *
       *       {
       *
       *         "match":{
       *
       *           "key":"likes_dogs",
       *
       *           "value":[true]
       *
       *         }
       *
       *       },
       *
       *       {
       *
       *         "or": [ {"match":{"key":"name","value":["dog"]}},[{"match":{"key":"name","value":["cat"]}},{"wildcard":{"key":"color","value"["orange"]}] ]
       *
       *       },
       *
       *
       *
       *     ]
       */
      filters?: components["schemas"]["filterListItem"][];
      /** Add relevance to documents if they match conditions in this list. configure 'weight' on each condition to modify the relevance contribution. */
      relevanceBoosters?: components["schemas"]["relevanceBoosterItem"][];
    };
    /** Aggregate output. Recursive aggregate output is returned here. */
    outputAggregations: {
      [key: string]: {
        /** Most common field values and their counts. */
        results: { [key: string]: number };
        aggregates: components["schemas"]["outputAggregations"];
      };
    };
    SimpleSearchPostOutput: {
      /**
       * List of documents. List items are affected by page, pageSize, query, filters. Items order is affected by vectorSeachQuery, sort, textSort.
       *
       *     Example: [{"_id":"abcd","animal":"cat","price":10}, {"_id":"abcd","price":13}]
       */
      results: {
        /** Measures how closely a document matches on query and vectorSearchQuery. */
        _relevance: number;
        _chunk_results?: {
          [key: string]: {
            results: {
              _relevance?: number;
            }[];
            _relevance: number;
          };
        };
      }[];
      /**
       * Total number of documents matched in the dataset.
       *
       *     Example: 100
       */
      resultsSize: number;
      aggregates: components["schemas"]["outputAggregations"];
      /**
       * Dictionary of most common field values specified in "fieldsToAggregate".
       *
       *     Example: {"color":{"red":15,"blue":7,"green":1},"name":{"roger":10000,"bill":500,"jo":200}}
       */
      aggregations: { [key: string]: { [key: string]: number } };
      /**
       * Dictionary of field stats.
       *
       *     Fields to summarise is specified in "fieldsToAggregateStats" of request body.
       *
       *     Example: {"price":{"min":0.5,"max":2000,"avg":100,sum:"40230032","results":{"0":202,"100":43,"200":16}}}
       */
      aggregateStats: {
        [key: string]: {
          /** Maximum value of field. */
          max: number;
          /** Minimum value of field. */
          min: number;
          /** Average value of field. */
          avg: number;
          /** Sum of all field values. */
          sum: number;
        };
      };
    };
    SimpleSearchGetInput: unknown;
    SimpleSearchGetOutput: unknown;
  };
}

export interface operations {
  /**
   * When inserting the document you can specify your own id for a document by using the field name **"\_id"**.
   * For specifying your own vector use the suffix (ends with)  **"\_vector\_"** for the field name.
   * e.g. "product\_description\_vector\_"
   */
  Insert: {
    parameters: {
      path: {
        /** ID of dataset */
        dataset_id: string;
      };
    };
    responses: {
      /** successful operation */
      200: {
        content: {
          "application/json": components["schemas"]["InsertOutput"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["InsertInput"];
      };
    };
  };
  /**
   * When inserting documents you can specify your own id for a document by using the field name **"\_id"**.
   * For specifying your own vector use the suffix (ends with)  **"\_vector\_"** for the field name.
   * e.g. "product\_description\_vector\_"
   */
  BulkInsert: {
    parameters: {
      path: {
        /** ID of dataset */
        dataset_id: string;
      };
    };
    responses: {
      /** successful operation */
      200: {
        content: {
          "application/json": components["schemas"]["BulkInsertOutput"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["BulkInsertInput"];
      };
    };
  };
  /** Generate an upload and download url for temporary storing of large files. Used in conjunction with parse_blob to upload large datasets. */
  BlobAccessURLs: {
    parameters: {
      path: {
        /** ID of dataset */
        dataset_id: string;
      };
    };
    responses: {
      /** successful operation */
      200: {
        content: {
          "application/json": components["schemas"]["BlobAccessURLsOutput"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["BlobAccessURLsInput"];
      };
    };
  };
  /** Bulk insert a large number of documents by downloading a file using "download url". This bypasses the need to directly send documents to the api, as happens in BulkInsert. */
  ParseBlob: {
    parameters: {
      path: {
        /** ID of dataset */
        dataset_id: string;
      };
    };
    responses: {
      /** successful operation */
      200: {
        content: {
          "application/json": components["schemas"]["ParseBlobOutput"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["ParseBlobInput"];
      };
    };
  };
  /** Edit by providing a key value pair of fields you are adding or changing. */
  Update: {
    parameters: {
      path: {
        /** ID of dataset */
        dataset_id: string;
      };
    };
    responses: {
      /** successful operation */
      200: {
        content: {
          "application/json": components["schemas"]["UpdateOutput"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["UpdateInput"];
      };
    };
  };
  /** Edits documents by providing a key value pair of fields you are adding or changing, make sure to include the "_id" in the documents. */
  BulkUpdate: {
    parameters: {
      path: {
        /** ID of dataset */
        dataset_id: string;
      };
    };
    responses: {
      /** successful operation */
      200: {
        content: {
          "application/json": components["schemas"]["BulkUpdateOutput"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["BulkUpdateInput"];
      };
    };
  };
  /** Updates documents by filters. The updates to make to the documents that is returned by a filter. The updates should be specified in a format of {"field_name": "value"}. e.g. {"item.status" : "Sold Out"} */
  UpdateWhere: {
    parameters: {
      path: {
        /** ID of dataset */
        dataset_id: string;
      };
    };
    responses: {
      /** successful operation */
      200: {
        content: {
          "application/json": components["schemas"]["UpdateWhereOutput"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["UpdateWhereInput"];
      };
    };
  };
  /**
   * All properties supported in the POST body can be set in query parameters when using GET method.
   *     It returns the same response body SimpleSearchPost
   *
   *
   *     For example:
   *
   *     /latest/datasets/:dataset_id/simple_search?query=c&page=1&pageSize=1&fieldsToAggregate=["filter_field"]
   *
   *
   *
   *     Integer and string properies such as query, page, pageSize should be directly included as query parameters.
   *
   *     Object properties such as "fieldsToAggregate" should be stringified as JSON, then included as parameters.
   *
   *
   *
   *     For example, in javascript, use the following code to add an object query parameter:
   *
   *     "/latest/datasets/:dataset_id/simple_search?fieldsToAggregate="+JSON.stringify(["filter_field"])
   */
  SimpleSearchGet: {
    parameters: {
      path: {
        /** ID of dataset */
        dataset_id: string;
      };
    };
    responses: {
      /** successful operation */
      200: {
        content: {
          "application/json": components["schemas"]["SimpleSearchGetOutput"];
        };
      };
    };
  };
  /** SimpleSearch is an easy way to use vector search and traditional text matching to search your dataset. It also supports filtering, sorting and aggregating results. */
  SimpleSearchPost: {
    parameters: {
      path: {
        /** ID of dataset */
        dataset_id: string;
      };
    };
    responses: {
      /** successful operation */
      200: {
        content: {
          "application/json": components["schemas"]["SimpleSearchPostOutput"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["SimpleSearchPostInput"];
      };
    };
  };
}

export interface external {}
