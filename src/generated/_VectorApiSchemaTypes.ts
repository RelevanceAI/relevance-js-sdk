/**
 * This file was auto-generated by openapi-typescript.
 * Do not make direct changes to the file.
 */

export interface paths {
  "/admin/request_read_api_key": {
    /** Creates a read only key for your project. Make sure to save the api key somewhere safe. When doing a search the admin username should still be used. */
    post: operations["request_read_api_key_api_admin_request_read_api_key_post"];
  };
  "/admin/copy_foreign_dataset": {
    /** Copy a dataset from another user's projects into your project. This is considered a project job */
    post: operations["copy_foreign_dataset_admin_copy_foreign_dataset_post"];
  };
  "/datasets/create": {
    /**
     * A dataset can store documents to be **searched, retrieved, filtered and aggregated** (similar to Collections in MongoDB, Tables in SQL, Indexes in ElasticSearch).
     *
     * A powerful and core feature of VecDB is that you can store both your metadata and vectors in the same document.
     * When specifying the schema of a dataset and inserting your own vector use the suffix (ends with) **"\_vector\_"** for the field name, and specify the length of the vector in dataset_schema.
     *
     * For example:
     *
     *     {
     *         "product_image_vector_": 1024,
     *         "product_text_description_vector_" : 128
     *     }
     *
     * These are the field types supported in our datasets: **["text", "numeric", "date", "dict", "chunks", "vector", "chunkvector"]**.
     *
     * For example:
     *
     *     {
     *         "product_text_description" : "text",
     *         "price" : "numeric",
     *         "created_date" : "date",
     *         "product_texts_chunk_": "chunks",
     *         "product_text_chunkvector_" : 1024
     *     }
     *
     * You don't have to specify the schema of every single field when creating a dataset, as VecDB will automatically detect the appropriate data type for each field (vectors will be automatically identified by its **"\_vector\_"** suffix). Infact you also don't always have to use this endpoint to create a dataset as **\/datasets/bulk_insert** will infer and create the dataset and schema as you insert new documents.
     *
     * Note:
     *  * A dataset name/id can only contain undercase letters, dash, underscore and numbers.
     *  * "\_id" is reserved as the key and id of a document.
     *  * Once a schema is set for a dataset it cannot be altered. If it has to be altered, utlise the copy dataset endpoint.
     *
     * For more information about vectors check out the 'Vectorizing' section, **\/services/search/vector** or out blog at [https://relevance.ai/blog](https://relevance.ai/blog).
     * For more information about chunks and chunk vectors check out **\/services/search/chunk**.
     */
    post: operations["create_dataset_api_datasets_create_post"];
  };
  "/datasets/infer_schema": {
    /** Create a dataset and infer its schema from a single document. Refer to _/datasets_/create for more about how a dataset is created. */
    post: operations["infer_schema_api_datasets_infer_schema_post"];
  };
  "/datasets/{dataset_id}/schema": {
    /** Returns the schema of a dataset. Refer to _/datasets_/create for different field types available in a VecDB schema. */
    get: operations["schema_api_datasets__dataset_id__schema_get"];
  };
  "/datasets/delete": {
    /** Deletes a dataset. */
    post: operations["delete_dataset_api_datasets_delete_post"];
  };
  "/datasets/list": {
    /** List all datasets in a project that you are authorized to read/write. */
    get: operations["list_datasets_api_datasets_list_get"];
    /**
     * Returns a page of datasets and in detail the dataset's associated information that you are authorized to read/write. The information includes:
     *
     * | Information      | Description |
     * | ----------- | ----------- |
     * | schema      | Data schema of a dataset. (returns same data as **\/dataset/{dataset_id}/schema**) |
     * | metadata   | Metadata of a dataset. (returns same data as **\/dataset/{dataset_id}/metadata**) |
     * | stats   | Statistics of number of documents and size of a dataset. (returns same data as **\/dataset/{dataset_id}/monitor/stats**) |
     * | vector_health   | Number of zero vectors stored. (returns same data as **\/dataset/{dataset_id}/monitor/health**) |
     * | schema_stats   | Fields and number of documents missing/not missing for that field. (returns same data as **\/dataset/{dataset_id}/monitor/stats**) |
     * | active_jobs   | All active jobs/tasks on the dataset. (returns same data as **\/dataset/{dataset_id}/tasks/list**) |
     */
    post: operations["list_collections_api_datasets_list_post"];
  };
  "/datasets/search": {
    /** Search datasets by their names with a traditional keyword search. */
    get: operations["search_datasets_api_datasets_search_get"];
  };
  "/datasets/{dataset_id}/facets": {
    /** Takes a high level aggregation of every field, return their unique values and frequencies. This is used to help create the filter bar for search. */
    post: operations["facets_api_datasets__dataset_id__facets_post"];
  };
  "/datasets/{dataset_id}/clone": {
    /** Clone a dataset into a new dataset. You can use this to rename fields and change data schemas. This is considered a project job. */
    post: operations["clone_dataset_api_datasets__dataset_id__clone_post"];
  };
  "/datasets/{dataset_id}/store_encoders_pipeline": {
    /**  */
    post: operations["store_encoders_pipeline_api_datasets__dataset_id__store_encoders_pipeline_post"];
  };
  "/datasets/{dataset_id}/vector_mappings": {
    /** Retrieve the mapping of vectors generated through fields, dictionary, array, etc */
    get: operations["dataset_vector_mappings_api_datasets__dataset_id__vector_mappings_get"];
    /** Retrieve the mapping of vectors generated through fields, dictionary, array, etc. */
    post: operations["dataset_vector_mappings_api_post_datasets__dataset_id__vector_mappings_post"];
  };
  "/datasets/{dataset_id}/documents/insert": {
    /**
     * * When inserting the document you can optionally specify your own id for a document by using the field name **"\_id"**, if not specified a random id is assigned.
     *  * When inserting or specifying vectors in a document use the suffix (ends with)  **"\_vector\_"** for the field name. e.g. "product\_description\_vector\_".
     *  * When inserting or specifying chunks in a document the suffix (ends with)  **"\_chunk\_"** for the field name. e.g. "products\_chunk\_".
     *  * When inserting or specifying chunk vectors in a document's chunks use the suffix (ends with)  **"\_chunkvector\_"** for the field name. e.g. "products_chunk_.product\_description\_chunkvector\_".
     * For multiple document insert version of this request use **\/datasets/{dataset_id}/documents/bulk_insert**.
     */
    post: operations["insert_api_datasets__dataset_id__documents_insert_post"];
  };
  "/datasets/{dataset_id}/documents/bulk_insert": {
    /**
     * * When inserting the document you can optionally specify your own id for a document by using the field name **"\_id"**, if not specified a random id is assigned.
     *  * When inserting or specifying vectors in a document use the suffix (ends with)  **"\_vector\_"** for the field name. e.g. "product\_description\_vector\_".
     *  * When inserting or specifying chunks in a document the suffix (ends with)  **"\_chunk\_"** for the field name. e.g. "products\_chunk\_".
     *  * When inserting or specifying chunk vectors in a document's chunks use the suffix (ends with)  **"\_chunkvector\_"** for the field name. e.g. "products_chunk_.product\_description\_chunkvector\_".
     *
     * Try to keep each batch of documents to insert under 200mb to avoid the insert timing out. For single document insert version of this request use **\/datasets/{dataset_id}/documents/insert**.
     */
    post: operations["bulk_insert_api_datasets__dataset_id__documents_bulk_insert_post"];
  };
  "/datasets/{dataset_id}/documents/get": {
    /** Retrieve a document by its ID ("_id" field). This will retrieve the document faster than a filter applied on the "_id" field. For multiple id lookup version of this request use **\/datasets/{dataset_id}/documents/bulk_get**. */
    get: operations["id_lookup_api_datasets__dataset_id__documents_get_get"];
  };
  "/datasets/{dataset_id}/documents/bulk_get": {
    /** Retrieve documents by their IDs ("_id" field). This will retrieve the documents faster than a filter applied on the "_id" field. For single id lookup version of this request use **\/datasets/{dataset_id}/documents/get**. */
    post: operations["bulk_id_lookup_api_datasets__dataset_id__documents_bulk_get_post"];
  };
  "/datasets/{dataset_id}/documents/list": {
    /** Retrieve documents from a specified dataset. Cursor is provided to retrieve even more documents. Loop through it to retrieve all documents in the dataset. For pagination support refer to **\/datasets/{dataset_id}/documents/paginate**. */
    get: operations["retrieve_documents_api_datasets__dataset_id__documents_list_get"];
  };
  "/datasets/{dataset_id}/documents/get_where": {
    /**
     * Retrieve documents with filters.
     * Cursor is provided to retrieve even more documents. Loop through it to retrieve all documents in the database.
     * Filter is used to retrieve documents that match the conditions set in a filter query. This is used in advance search to filter the documents that are searched.
     *
     * The filters query is a json body that follows the schema of:
     *
     *     [
     *         {'field' : <field to filter>, 'filter_type' : <type of filter>, "condition":"==", "condition_value":"america"},
     *         {'field' : <field to filter>, 'filter_type' : <type of filter>, "condition":">=", "condition_value":90},
     *     ]
     *
     * These are the available filter_type types: **["contains", "category", "categories", "exists", "date", "numeric", "ids"]**
     *
     * 1. **"contains"**: for filtering documents that contains a string.
     *         {'field' : 'item_brand', 'filter_type' : 'contains', "condition":"==", "condition_value": "samsu"}
     * 2. **"exact_match"/"category"**: for filtering documents that matches a string or list of strings exactly.
     *         {'field' : 'item_brand', 'filter_type' : 'category', "condition":"==", "condition_value": "sumsung"}
     * 3. **"categories"**: for filtering documents that contains any of a category from a list of categories.
     *         {'field' : 'item_category_tags', 'filter_type' : 'categories', "condition":"==", "condition_value": ["tv", "smart", "bluetooth_compatible"]}
     * 4. **"exists"**: for filtering documents that contains a field.
     *         {'field' : 'purchased', 'filter_type' : 'exists', "condition":"==", "condition_value":" "}
     * If you are looking to filter for documents where a field doesn't exist, run this:
     *         {'field' : 'purchased', 'filter_type' : 'exists', "condition":"!=", "condition_value":" "}
     * 5. **"date"**: for filtering date by date range.
     *         {'field' : 'insert_date_', 'filter_type' : 'date', "condition":">=", "condition_value":"2020-01-01"}
     * 6. **"numeric"**: for filtering by numeric range.
     *         {'field' : 'price', 'filter_type' : 'numeric', "condition":">=", "condition_value":90}
     * 7. **"ids"**: for filtering by document ids.
     *         {'field' : 'ids', 'filter_type' : 'ids', "condition":"==", "condition_value":["1", "10"]}
     * 8. **"or"**: for filtering with multiple conditions
     *         {'filter_type' : 'or',
     * "condition_value": [{'field' : 'price', 'filter_type' : 'numeric', "condition":"<=", "condition_value":90},
     * {'field' : 'price', 'filter_type' : 'numeric', "condition":">=", "condition_value":150}]}
     *
     * These are the available conditions:
     *
     *     "==", "!=", ">=", ">", "<", "<="
     *
     * If you are looking to combine your filters with multiple ORs, simply add the following inside the query
     * `{"strict":"must_or"}`.
     *
     * For pagination support refer to **\/datasets/{dataset_id}/documents/paginate**.
     */
    post: operations["retrieve_documents_with_filters_api_datasets__dataset_id__documents_get_where_post"];
  };
  "/datasets/{dataset_id}/documents/paginate": {
    /** Retrieve documents with filters and support for pagination. For more information about filters refer to **\/datasets/{dataset_id}/documents/get_where**. */
    post: operations["retrieve_documents_with_filters_api_datasets__dataset_id__documents_paginate_post"];
  };
  "/datasets/{dataset_id}/documents/get_missing": {
    /** Look up in bulk if the ids exists in the dataset, returns all the missing one as a list. */
    get: operations["bulk_missing_id_api_datasets__dataset_id__documents_get_missing_get"];
  };
  "/datasets/{dataset_id}/documents/update": {
    /** Edit by providing a key value pair of fields you are adding or changing. For update multiple documents refer to **\/datasets/{dataset_id}/documents/bulk_update**. */
    post: operations["update_document_api_datasets__dataset_id__documents_update_post"];
  };
  "/datasets/{dataset_id}/documents/bulk_update": {
    /** Edits documents by providing a key value pair of fields you are adding or changing, make sure to include the "_id" in the documents. For update a single document refer to **\/datasets/{dataset_id}/documents/update** or updating documents by filters refer to **\/datasets/{dataset_id}/documents/update_where**. */
    post: operations["bulk_update_documents_api_datasets__dataset_id__documents_bulk_update_post"];
  };
  "/datasets/{dataset_id}/documents/delete": {
    /** Delete a document by its id. For deleting multiple documents refer to **\/datasets/{dataset_id}/documents/bulk_delete**. */
    post: operations["delete_api_datasets__dataset_id__documents_delete_post"];
  };
  "/datasets/{dataset_id}/documents/bulk_delete": {
    /** Delete a list of documents by their IDs. For deleting a single document refer to **\/datasets/{dataset_id}/documents/delete** or deleting documents by filters refer to **\/datasets/{dataset_id}/documents/delete_where**. */
    post: operations["bulk_delete_api_datasets__dataset_id__documents_bulk_delete_post"];
  };
  "/datasets/{dataset_id}/documents/delete_fields": {
    /** Delete fields in a document in a dataset by its id. */
    post: operations["delete_fields_api_datasets__dataset_id__documents_delete_fields_post"];
  };
  "/datasets/{dataset_id}/documents/update_where": {
    /** Updates documents by filters. The updates to make to the documents that is returned by a filter. The updates should be specified in a format of {"field_name": "value"}. e.g. {"item.status" : "Sold Out"}. For more information about filters refer to **\/datasets/{dataset_id}/documents/get_where**. */
    post: operations["update_by_filters_api_datasets__dataset_id__documents_update_where_post"];
  };
  "/datasets/{dataset_id}/documents/delete_where": {
    /** Delete documents by filters. For more information about filters refer to **\/datasets/{dataset_id}/documents/get_where**. */
    post: operations["delete_by_filters_api_datasets__dataset_id__documents_delete_where_post"];
  };
  "/datasets/{dataset_id}/metadata": {
    /** Retreives metadata about a dataset. Notably description, data source, etc */
    get: operations["collection_metadata_api_datasets__dataset_id__metadata_get"];
    /** Edit and add metadata about a dataset. Notably description, data source, etc */
    post: operations["post_collection_metadata_api_datasets__dataset_id__metadata_post"];
  };
  "/datasets/{dataset_id}/monitor/stats": {
    get: operations["dataset_schema_stats_api_datasets__dataset_id__monitor_stats_get"];
  };
  "/datasets/{dataset_id}/monitor/health": {
    /** Gives you a summary of the health of your vectors, e.g. how many documents with vectors are missing, how many documents with zero vectors */
    get: operations["dataset_vector_health_api_datasets__dataset_id__monitor_health_get"];
  };
  "/datasets/{dataset_id}/monitor/usage": {
    /**
     * Aggregate the logs for a dataset
     *
     * The response returned has the following fields.
     *
     *     [{'frequency': 958, 'insert_date': 1630159200000},...]
     */
    post: operations["aggregate_logs_api_datasets__dataset_id__monitor_usage_post"];
  };
  "/datasets/{dataset_id}/tasks/create": {
    /** Tasks unlock the power of VecDb AI by adding a lot more new functionality with a flexible way of searching. */
    post: operations["tasks_api_datasets__dataset_id__tasks_create_post"];
  };
  "/datasets/{dataset_id}/tasks/list": {
    /** List and get a history of all the jobs and its job_id, parameters, start time, etc. */
    get: operations["list_collection_jobs_api_datasets__dataset_id__tasks_list_get"];
  };
  "/datasets/{dataset_id}/tasks/{task_id}/status": {
    /** Get status of a collection level job. Whether its starting, running, failed or finished. */
    get: operations["tasks_status_api_datasets__dataset_id__tasks__task_id__status_get"];
  };
  "/datasets/{dataset_id}/vectorize": {
    /** Queue the encoding of a dataset using the method given by model_id. */
    post: operations["encode_by_model_api_datasets__dataset_id__vectorize_post"];
  };
  "/datasets/{dataset_id}/task_status": {
    /**
     * Check the status of an existing encoding task on the given dataset.
     *
     * The required task_id was returned in the original encoding request
     * such as /vectorize.
     */
    get: operations["task_status_by_model_api_datasets__dataset_id__task_status_get"];
  };
  "/datasets/{dataset_id}/tasks": {
    /**
     * List the tasks being encoded for the dataset_id that
     * you are authorized to read.
     *
     * If dataset_id is the wildcard "*" then all tasks for the
     * user project are listed.
     */
    get: operations["list_tasks_api_datasets__dataset_id__tasks_get"];
  };
  "/services/search/vector": {
    /**
     * Allows you to leverage vector similarity search to create a semantic search engine.
     * Powerful features of VecDB vector search:
     *  1. Multivector search that allows you to search with multiple vectors and give each vector a different weight. e.g. Search with a product image vector and text description vector to find the most similar products by what it looks like and what its described to do. You can also give weightings of each vector field towards the search, e.g. image\_vector\_ weights 100%, whilst description\_vector\_ 50%.
     *
     *     An example of a simple multivector query:
     *
     *         [
     *             {"vector": [0.12, 0.23, 0.34], "fields": ["name_vector_"], "alias":"text"},
     *             {"vector": [0.45, 0.56, 0.67], "fields": ["image_vector_"], "alias":"image"},
     *         ]
     *
     *     An example of a weighted multivector query:
     *
     *         [
     *             {"vector": [0.12, 0.23, 0.34], "fields": {"name_vector_":0.6}, "alias":"text"},
     *             {"vector": [0.45, 0.56, 0.67], "fields": {"image_vector_"0.4}, "alias":"image"},
     *         ]
     *
     *     An example of a weighted multivector query with multiple fields for each vector:
     *
     *         [
     *             {"vector": [0.12, 0.23, 0.34], "fields": {"name_vector_":0.6, "description_vector_":0.3}, "alias":"text"},
     *             {"vector": [0.45, 0.56, 0.67], "fields": {"image_vector_"0.4}, "alias":"image"},
     *         ]
     *
     *  2. Utilise faceted search with vector search. For information on how to apply facets/filters checkout **\/datasets/{dataset_id}/documents/get_where**.
     *
     *  3. Sum Fields option to adjust whether you want multiple vectors to be combined in the scoring or compared in the scoring. e.g. image\_vector\_ + text\_vector\_ or image\_vector\_ vs text\_vector\_.
     *
     *     setting `sum_fields=True`:
     *      * Multi-vector search allows you to obtain search scores by taking the sum of these scores.
     *      * TextSearchScore + ImageSearchScore = SearchScore
     *      * We then rank by the new SearchScore, so for searching 1000 documents there will be 1000 search scores and results
     *
     *     setting `sum_fields=False`:
     *      * Multi vector search but not summing the score, instead including it in the comparison!
     *      * TextSearchScore = SearchScore1
     *      * ImagSearcheScore = SearchScore2
     *      * We then rank by the 2 new SearchScore, so for searching 1000 documents there should be 2000 search scores and results.
     *
     *  4. Personalization with positive and negative document ids.
     *
     *     For more information about the positive and negative document ids to personalize checkout the **\/services/recommend/vector** endpoint.
     *
     * For more even more advanced configuration and customisation of vector search, reach out to us at dev@relevance.ai and learn about our new advanced_vector_search.
     */
    post: operations["vector_search_api_services_search_vector_post"];
  };
  "/services/search/traditional": {
    /**
     * Traditional Faceted Keyword Search with edit distance/fuzzy matching.
     *
     * For information on how to apply facets/filters checkout **\/datasets/{dataset_id}/documents/get_where**.
     *
     * For information on how to construct the facets section for your search bar checkout out **\/datasets/{dataset_id}/facets**.
     */
    post: operations["traditional_search_api_services_search_traditional_post"];
  };
  "/services/search/hybrid": {
    /**
     * Combine the best of both traditional keyword faceted search with semantic vector search to create the best search possible.
     *
     * For information on how to use vector search **\/services/search/vector**.
     *
     * For information on how to use traditional keyword faceted search **\/services/search/traditional**.
     */
    post: operations["hybrid_search_api_services_search_hybrid_post"];
  };
  "/services/search/semantic": {
    /**
     * A more automated hybrid search with a few extra things that automatically adjusts some of the key parameters for more automated and good out of the box results.
     *
     * For information on how to configure semantic search checkout **\/services/search/hybrid**.
     */
    post: operations["hybrid_search_api_services_search_semantic_post"];
  };
  "/services/search/chunk": {
    /**
     * Chunks are data that has been divided into different units. e.g. A paragraph is made of many sentence chunks, a sentence is made of many word chunks, an image frame in a video. By searching through chunks you can pinpoint more specifically where a match is occuring.
     * When creating a chunk in your document use the suffix "_chunk_" and "_chunkvector_". An example of a document with chunks:
     *
     *     {
     *         "_id" : "123",
     *         "title" : "Lorem Ipsum Article",
     *         "description" : "Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.",
     *         "description_vector_" : [1.1, 1.2, 1.3],
     *         "description_sentence_chunk_" : [
     *             {"sentence_id" : 0, "sentence_chunkvector_" : [0.1, 0.2, 0.3], "sentence" : "Lorem Ipsum is simply dummy text of the printing and typesetting industry."},
     *             {"sentence_id" : 1, "sentence_chunkvector_" : [0.4, 0.5, 0.6], "sentence" : "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book."},
     *             {"sentence_id" : 2, "sentence_chunkvector_" : [0.7, 0.8, 0.9], "sentence" : "It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged."},
     *         ]
     *     }
     *
     * For combining chunk search with other search checkout **\/services/search/advanced_chunk**.
     */
    post: operations["chunk_search_api_services_search_chunk_post"];
  };
  "/services/search/multistep_chunk": {
    /**
     * Multistep chunk search involves a vector search followed by chunk search, used to accelerate chunk searches or to identify context before delving into relevant chunks. e.g. Search against the paragraph vector first then sentence chunkvector after.
     *
     * For more information about chunk search checkout **\/services/search/chunk**.
     *
     * For more information about vector search checkout **\/services/search/vector**.
     */
    post: operations["advanced_multistep_chunk_search_api_services_search_multistep_chunk_post"];
  };
  "/services/search/advanced_chunk": {
    /**
     * A more advanced chunk search to be able to combine vector search and chunk search in many different ways.
     *
     *     chunk_query = {
     *         "chunk" : "some.test",
     *         "queries" : [
     *             {"vector" : vec1, "fields": {"some.test.some_chunkvector_":1},
     *             "traditional_query" : {"text":"python", "fields" : ["some.test.test_words"], "traditional_weight": 0.3},
     *             "metric" : "cosine"},
     *             {"vector" : vec, "fields": ["some.test.tt.some_other_chunkvector_"],
     *             "traditional_query" : {"text":"jumble", "fields" : ["some.test.test_words"], "traditional_weight": 0.3},
     *             "metric" : "cosine"},
     *         ]
     *     }
     * Example 2 (combines normal vector search with chunk search):
     *
     *     chunk_query = {
     *         "queries" : [
     *             {
     *                 "queries": [
     *                     {
     *                         "vector": vec1,
     *                         "fields": {
     *                             "some.test.some_chunkvector_": 0.9
     *                         },
     *                         "traditional_query": {
     *                             "text": "python",
     *                             "fields": [
     *                                 "some.test.test_words"
     *                             ],
     *                             "traditional_weight": 0.3
     *                         },
     *                         "metric": "cosine"
     *                     }
     *                 ],
     *                 "chunk": "some.test",
     *             },
     *             {
     *                 "vector" : vec,
     *                 "fields": {
     *                     ".some_vector_" : 0.1},
     *                     "metric" : "cosine"
     *                     },
     *             ]
     *         }
     */
    post: operations["advanced_chunk_search_api_services_search_advanced_chunk_post"];
  };
  "/services/search/advanced_multistep_chunk": {
    /**
     * Performs a vector hybrid search and then an advanced chunk search.
     *
     * Chunk Search allows one to search through chunks inside a document. The major difference between chunk search and normal search in Vector AI is that it relies on the _chunkvector_ field.
     * Chunk Vector Search. Search with a multiple chunkvectors for the most similar documents.
     * Chunk search also supports filtering to only search through filtered results and facets to get the overview of products available when a minimum score is set.
     *
     * Example 1 (Hybrid chunk search):
     *
     *     chunk_query1= {
     *         "chunk" : "some.test",
     *         "queries" : [
     *             {"vector" : vec1, "fields": {"some.test.some_chunkvector_":1},
     *             "traditional_query" : {"text":"python", "fields" : ["some.test.test_words"], "traditional_weight": 0.3},
     *             "metric" : "cosine"},
     *             {"vector" : vec, "fields": ["some.test.tt.some_other_chunkvector_"],
     *             "traditional_query" : {"text":"jumble", "fields" : ["some.test.test_words"], "traditional_weight": 0.3},
     *             "metric" : "cosine"},
     *         ]
     *     }
     *
     * Example 2 (combines normal vector search with chunk search):
     *
     *     chunk_query1= {
     *     "queries" : [
     *         {
     *             "queries": [
     *                 {
     *                     "vector": vec1,
     *                     "fields": {
     *                         "some.test.some_chunkvector_": 0.9
     *                     },
     *                     "traditional_query": {
     *                         "text": "python",
     *                         "fields": [
     *                             "some.test.test_words"
     *                         ],
     *                         "traditional_weight": 0.3
     *                     },
     *                     "metric": "cosine"
     *                 }
     *             ],
     *             "chunk": "some.test",
     *         },
     *         {
     *             "vector" : vec,
     *             "fields": {
     *                 ".some_vector_" : 0.1},
     *                 "metric" : "cosine"
     *                 },
     *         ]
     *     }
     */
    post: operations["advanced_multistep_chunk_search_api_services_search_advanced_multistep_chunk_post"];
  };
  "/services/search/diversity": {
    /**
     * This will first perform an advanced search and then cluster the top X (page_size) search results.
     * Results are returned as such:
     * Once you have the clusters:
     *
     * ```
     * Cluster 0: [A, B, C]
     * Cluster 1: [D, E]
     * Cluster 2: [F, G]
     * Cluster 3: [H, I]
     * ```
     * (Note, each cluster is ordered by highest to lowest search score.
     *
     * This intermediately returns:
     *
     * ```
     * results_batch_1: [A, H, F, D] (ordered by highest search score)
     * results_batch_2: [G, E, B, I] (ordered by highest search score)
     * results_batch_3: [C]
     * ```
     *
     * This then returns the final results:
     *
     * ```
     * results: [A, H, F, D, G, E, B, I, C]
     */
    post: operations["vector_search_api_services_search_diversity_post"];
  };
  "/services/recommend/vector": {
    /**
     * Vector Search based recommendations are done by extracting the vectors of the documents ids specified performing some vector operations and then searching the dataset with the resultant vector.
     *
     * This allows us to not only do recommendations but personalized and weighted recommendations here are a couple of different scenarios and what the queries would look like for those:
     *
     * 1. Recommendations Personalized by single liked product:
     *
     *     `positive_document_ids=['A']`
     *
     *     -> Document ID A Vector = Search Query
     *
     * 2. Recommendations Personalized by multiple liked product:
     *
     *     `positive_document_ids=['A', 'B']`
     *
     *     -> Document ID A Vector + Document ID B Vector = Search Query
     *
     * 3. Recommendations Personalized by multiple liked product and disliked products:
     *
     *     `positive_document_ids=['A', 'B'], negative_document_ids=['C', 'D']`
     *
     *     -> (Document ID A Vector + Document ID B Vector) - (Document ID C Vector + Document ID C Vector) = Search Query
     *
     * 4. Recommendations Personalized by multiple liked product and disliked products with weights:
     *
     *     `positive_document_ids={'A':0.5, 'B':1}, negative_document_ids={'C':0.6, 'D':0.4}`
     *
     *     -> (Document ID A Vector * 0.5 + Document ID B Vector * 1) - (Document ID C Vector * 0.6 + Document ID D Vector * 0.4) = Search Query
     *
     * You can change the operator between vectors with vector_operation:
     *
     * e.g. `positive_document_ids=['A', 'B'], negative_document_ids=['C', 'D'], vector_operation='multiply'`
     *
     * -> (Document ID A Vector * Document ID B Vector) - (Document ID C Vector * Document ID D Vector) = Search Query
     */
    post: operations["vector_recommend_api_services_recommend_vector_post"];
  };
  "/services/recommend/diversity": {
    /**
     * Vector Search based recommendations are done by extracting the vectors of the documents ids specified performing some vector operations and then searching the dataset with the resultant vector.
     *
     * This allows us to not only do recommendations but personalized and weighted recommendations.
     *
     * Diversity recommendation increases the variety within the recommendations via clustering. Search results are clustered and the top k items in each cluster are selected.
     * The main clustering parameters are
     * `cluster_vector_field` and `n_clusters`, the vector field on which to perform clustering and number of clusters respectively.
     *
     * Here are a couple of different scenarios and what the queries would look like for those:
     *
     * 1. Recommendations Personalized by single liked product:
     *
     *     `positive_document_ids=['A']`
     *
     *     -> Document ID A Vector = Search Query
     *
     * 2. Recommendations Personalized by multiple liked product:
     *
     *     `positive_document_ids=['A', 'B']`
     *
     *     -> Document ID A Vector + Document ID B Vector = Search Query
     *
     * 3. Recommendations Personalized by multiple liked product and disliked products:
     *
     *     `positive_document_ids=['A', 'B'], negative_document_ids=['C', 'D']`
     *
     *     -> (Document ID A Vector + Document ID B Vector) - (Document ID C Vector + Document ID C Vector) = Search Query
     *
     * 4. Recommendations Personalized by multiple liked product and disliked products with weights:
     *
     *     `positive_document_ids={'A':0.5, 'B':1}, negative_document_ids={'C':0.6, 'D':0.4}`
     *
     *     -> (Document ID A Vector * 0.5 + Document ID B Vector * 1) - (Document ID C Vector * 0.6 + Document ID D Vector * 0.4) = Search Query
     *
     * You can change the operator between vectors with vector_operation:
     *
     * e.g. `positive_document_ids=['A', 'B'], negative_document_ids=['C', 'D'], vector_operation='multiply'`
     *
     * -> (Document ID A Vector * Document ID B Vector) - (Document ID C Vector * Document ID D Vector) = Search Query
     */
    post: operations["vector_diversity_recommend_api_services_recommend_diversity_post"];
  };
  "/services/aggregate/aggregate": {
    /**
     * Aggregation/Groupby of a collection using an aggregation query.
     * The aggregation query is a json body that follows the schema of:
     *
     *     {
     *         "groupby" : [
     *             {"name": <alias>, "field": <field in the collection>, "agg": "category"},
     *             {"name": <alias>, "field": <another groupby field in the collection>, "agg": "numeric"}
     *         ],
     *         "metrics" : [
     *             {"name": <alias>, "field": <numeric field in the collection>, "agg": "avg"}
     *             {"name": <alias>, "field": <another numeric field in the collection>, "agg": "max"}
     *         ]
     *     }
     *     For example, one can use the following aggregations to group score based on region and player name.
     *     {
     *         "groupby" : [
     *             {"name": "region", "field": "player_region", "agg": "category"},
     *             {"name": "player_name", "field": "name", "agg": "category"}
     *         ],
     *         "metrics" : [
     *             {"name": "average_score", "field": "final_score", "agg": "avg"},
     *             {"name": "max_score", "field": "final_score", "agg": "max"},
     *             {'name':'total_score','field':"final_score", 'agg':'sum'},
     *             {'name':'average_deaths','field':"final_deaths", 'agg':'avg'},
     *             {'name':'highest_deaths','field':"final_deaths", 'agg':'max'},
     *         ]
     *     }
     * - "groupby" is the fields you want to split the data into. These are the available groupby types:
     *     - category" : groupby a field that is a category
     *     - numeric: groupby a field that is a numeric
     *     - wordcloud: groupby the words. You can also additionally include stop words in your aggregation
     *     if you add `remove_words` as a field.
     *
     *     {
     *         "name": "wordcloud_research",
     *         "field": "title",
     *         "agg": "wordcloud",
     *         "remove_words": ["learning"]
     *     }
     *
     *
     * - "metrics" is the fields you want to metrics you want to calculate in each of those, every aggregation includes a frequency metric. These are the available metric types:
     *     - "avg", "max", "min", "sum", "cardinality"
     *
     * The response returned has the following in descending order.
     *
     * IF you want to return documents, specify a "group_size" parameter and a "select_fields" parameter if you want to limit the specific fields chosen.
     * This looks as such:
     *
     *     {
     *       'groupby':[
     *         {'name':'Manufacturer','field':'manufacturer','agg':'category',
     *         'group_size': 10, 'select_fields': ["name"]},
     *       ],
     *       'metrics':[
     *         {'name':'Price Average','field':'price','agg':'avg'},
     *       ],
     *     }
     *
     *     {"title": {"title": "books", "frequency": 200, "documents": [{...}, {...}]}, {"title": "books", "frequency": 100, "documents": [{...}, {...}]}}
     */
    post: operations["aggregate_v2_api_services_aggregate_aggregate_post"];
  };
  "/services/cluster/centroids/list": {
    /** Retrieves a list of cluster centroids */
    get: operations["cluster_centroids_api_services_cluster_centroids_list_get"];
    /** Retrieves a list of cluster centroids */
    post: operations["cluster_centroids_api_v2_services_cluster_centroids_list_post"];
  };
  "/services/cluster/centroids/get": {
    /** Retrieve the cluster centroids by IDs */
    get: operations["cluster_centroids_get_api_services_cluster_centroids_get_get"];
    /** Retrieve the cluster centroids by IDs */
    post: operations["cluster_centroids_get_api_services_cluster_centroids_get_post"];
  };
  "/services/cluster/centroids/insert": {
    /** Insert your own cluster centroids for it to be used in approximate search settings and cluster aggregations. */
    post: operations["insert_cluster_centroids_2_api_services_cluster_centroids_insert_post"];
  };
  "/services/cluster/centroids/update": {
    /** Update a centroid by ID */
    post: operations["update_centroids_api_v2_services_cluster_centroids_update_post"];
  };
  "/services/cluster/centroids/{centroid_id}/delete": {
    /** Delete a centroid by ID */
    get: operations["delete_centroids_api_services_cluster_centroids__centroid_id__delete_get"];
    /** Delete a centroid by ID */
    post: operations["delete_centroids_api_services_cluster_centroids__centroid_id__delete_post"];
  };
  "/services/cluster/centroids/delete": {
    /** Delete centroids by dataset ID, vector field and alias */
    post: operations["cluster_centroids_delete_api_services_cluster_centroids_delete_post"];
  };
  "/services/cluster/centroids/documents": {
    /** Retrieve the cluster centroids by IDs */
    post: operations["cluster_centroids_get_api_services_cluster_centroids_documents_post"];
  };
  "/services/cluster/centroids/metadata": {
    /** Retrieves metadata about a dataset. notably description, data source, etc */
    get: operations["centroids_metadata_get_api_services_cluster_centroids_metadata_get"];
    /** You can store the metadata about your cluster here. */
    post: operations["centroids_metadata_post_api_v2_services_cluster_centroids_metadata_post"];
  };
  "/services/cluster/centroids/list_closest_to_center": {
    /** List of documents closest from the centre. */
    post: operations["centroids_list_closest_to_center_v2_services_cluster_centroids_list_closest_to_center_post"];
  };
  "/services/cluster/centroids/list_furthest_from_center": {
    /** List of documents from from the centre */
    post: operations["centroids_list_furthest_from_center_v2_services_cluster_centroids_list_furthest_from_center_post"];
  };
  "/services/cluster/aggregate": {
    /**
     * Takes an aggregation query and gets the aggregate of each cluster in a collection. This helps you interpret each cluster and what is in them.
     *
     * Only can be used after a vector field has been clustered with /cluster.
     */
    post: operations["cluster_aggregate_api_v2_services_cluster_aggregate_post"];
  };
  "/services/cluster/facets": {
    /**
     * Takes a high level aggregation of every field and every cluster in a collection. This helps you interpret each cluster and what is in them.
     *
     * Only can be used after a vector field has been clustered with /cluster.
     */
    get: operations["advanced_cluster_facets_api_services_cluster_facets_get"];
  };
  "/services/cluster/list": {
    /** Get a list of cluster IDs based on the relevant information */
    get: operations["cluster_list_services_cluster_list_get"];
    /** Get a list of cluster IDs */
    post: operations["cluster_list_multi_services_cluster_list_post"];
  };
  "/services/tagger/tag": {
    /** Tag documents or vectors. */
    post: operations["tag_api_services_tagger_tag_post"];
  };
  "/services/tagger/diversity": {
    /** Tagging and then clustering the tags and returning one from each cluster (starting from the closest tag) */
    post: operations["cluster_and_tag_api_services_tagger_diversity_post"];
  };
  "/services/document_diff": {
    /** Recommend something via the API. */
    post: operations["vector_recommend_api_services_document_diff_post"];
  };
  "/services/prediction/regression/knn": {
    /** Predict using KNN regression. */
    post: operations["predict_knn_regression_api_services_prediction_regression_knn_post"];
  };
  "/services/prediction/regression/knn_from_results": {
    /** Predict using KNN regression from search results */
    post: operations["predict_knn_regression_from_search_results_api_services_prediction_regression_knn_from_results_post"];
  };
  "/services/encoders/numeric_fields": {
    /**
     * For example: we choose the fields ["height", "age", "weight"]
     *     document field: {"height":180, "age":40, "weight":70, "purchases":20, "visits": 12}
     *
     *     -> <Encode the fields to vectors> ->
     *
     * | height | age | weight |
     * |--------|-----|--------|
     * | 180    | 40  | 70     |
     *
     *     document vector: {"person_characteristics_vector_": [180, 40, 70]}
     */
    post: operations["encode_numeric_fields_api_services_encoders_numeric_fields_post"];
  };
  "/services/encoders/categories": {
    /**
     * For example: an array that represents a **movie's categories, field "movie_categories"**:
     *
     *     ["sci-fi", "thriller", "comedy"]
     *
     *     -> <Encode the arrays to vectors> ->
     *
     * | sci-fi | thriller | comedy | romance | drama |
     * |--------|----------|--------|---------|-------|
     * | 1      | 1        | 1      | 0       | 0     |
     *
     *     array vector: [1, 1, 1, 0, 0]
     */
    post: operations["encode_categories_api_services_encoders_categories_post"];
  };
  "/services/encoders/dictionary": {
    /**
     * For example: a dictionary that represents a **person's characteristics visiting a store, field "person_characteristics"**:
     *
     *     {"height":180, "age":40, "weight":70}
     *
     *     -> <Encode the dictionary to vector> ->
     *
     * | height | age | weight | purchases | visits |
     * |--------|-----|--------|-----------|--------|
     * | 180    | 40  | 70     | 0         | 0      |
     *
     *     dictionary vector: [180, 40, 70, 0, 0]
     */
    post: operations["encode_dictionary_api_services_encoders_dictionary_post"];
  };
  "/services/encoders/text": {
    /** Encode text */
    get: operations["encode_text_api_services_encoders_text_get"];
  };
  "/services/encoders/multi_text": {
    /** Encode text */
    get: operations["encode_text_api_services_encoders_multi_text_get"];
  };
  "/services/encoders/image": {
    /** Encode an image */
    post: operations["encode_image_api_services_encoders_image_post"];
  };
  "/services/encoders/textimage": {
    /** Encode text to make searchable with images */
    get: operations["encode_textimage_api_services_encoders_textimage_get"];
  };
  "/services/encoders/imagetext": {
    /** Encode an image to make searchable with text */
    get: operations["encode_imagetext_api_services_encoders_imagetext_get"];
  };
  "/services/encoders/encode": {
    /**
     * Receives a document and encode specified fields into vectors with provided model urls or model names.
     * e.g. [
     *     {"model_url" : "https://a_vector_model_url.com/encode_image_url", "body" : "url", "field": "thumbnail"},
     *     {"model_url" : "https://a_vector_model_url.com/encode_text", "body" : "text", "field": "short_description"},
     *     {"model_url" : "bert", "body" : "text", "field": "short_description", "alias":"bert"},
     * ]
     */
    post: operations["retrieve_documents_api_services_encoders_encode_post"];
  };
  "/services/encoders/bulk_encode": {
    /**
     * Gets multiple documents and encodes specified fields into vectors with provided model urls or model names.
     * e.g. [
     *     {"model_url" : "https://a_vector_model_url.com/encode_image_url", "body" : "url", "field": "thumbnail"},
     *     {"model_url" : "https://a_vector_model_url.com/encode_text", "body" : "text", "field": "short_description"},
     *     {"model_url" : "bert", "body" : "text", "field": "short_description", "alias":"bert"},
     * ]
     */
    post: operations["retrieve_documents_api_services_encoders_bulk_encode_post"];
  };
  "/services/wordclouds/wordclouds": {
    /** Get frequency n-gram frequency counter from the wordcloud. */
    post: operations["wordclouds_api_services_wordclouds_wordclouds_post"];
  };
  "/deployables/create": {
    /** Create a private deployable. */
    post: operations["deployable_create_api_deployables_create_post"];
  };
  "/deployables/{deployable_id}/share": {
    /**
     * Share a private deployable.
     *
     * The response should be {"status": "success"} or {"status": "failed"}
     */
    post: operations["deployable_update_shareable_api_deployables__deployable_id__share_post"];
  };
  "/deployables/{deployable_id}/private": {
    /**
     * Unshare a shared deployable, making it private.
     *
     * The response should be {"status": "success"} or {"status": "failed"}
     */
    post: operations["deployable_update_private_api_deployables__deployable_id__private_post"];
  };
  "/deployables/{deployable_id}/update": {
    post: operations["deployable_update_api_deployables__deployable_id__update_post"];
  };
  "/deployables/{deployable_id}/get": {
    /** Get a deployable. */
    get: operations["deployable_get_api_deployables__deployable_id__get_get"];
  };
  "/deployables/delete": {
    post: operations["deployable_delete_api_deployables_delete_post"];
  };
  "/deployables/list": {
    /** List all deployables. */
    get: operations["deployable_list_api_deployables_list_get"];
  };
}

export interface components {
  schemas: {
    /** Model for advanced chunk search */
    AdvancedChunkSearch: {
      /** Dataset IDs */
      dataset_ids: unknown[];
      /** Advanced chunk query */
      chunk_search_query: unknown[];
      /** Minimum score for similarity metric */
      min_score?: number;
      /** Size of each page of results */
      page_size?: number;
      /** Include vectors in the search results */
      include_vector?: boolean;
      /** Fields to include in the search results, empty array/list means all fields. */
      select_fields?: unknown[];
    };
    /** Base class for all abstractmodels */
    AdvancedDiversityRecommendBody: {
      /** Unique name of dataset */
      dataset_id: string;
      /** Positive document IDs to personalize the results with, this will retrive the vectors from the document IDs and consider it in the operation. */
      positive_document_ids?: Partial<{ [key: string]: number }> &
        Partial<string[]>;
      /** Negative document IDs to personalize the results with, this will retrive the vectors from the document IDs and consider it in the operation. */
      negative_document_ids?: Partial<{ [key: string]: number }> &
        Partial<string[]>;
      /** The vector field to search in. It can either be an array of strings (automatically equally weighted) (e.g. ['check_vector_', 'yellow_vector_']) or it is a dictionary mapping field to float where the weighting is explicitly specified (e.g. {'check_vector_': 0.2, 'yellow_vector_': 0.5}) */
      vector_fields: Partial<unknown[]> & Partial<{ [key: string]: number }>;
      /** Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate. */
      approximation_depth?: number;
      /** Aggregation for the vectors when using positive and negative document IDs, choose from ['mean', 'sum', 'min', 'max', 'divide', 'mulitple'] */
      vector_operation?: string;
      /** Whether to sum the multiple vectors similarity search score as 1 or seperate */
      sum_fields?: boolean;
      /** Size of each page of results */
      page_size?: number;
      /** Page of the results */
      page?: number;
      /** Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp'] */
      similarity_metric?: string;
      /** Fields to include in the facets, if [] then all */
      facets?: string[];
      /** Query for filtering the search results */
      filters?: { [key: string]: unknown }[];
      /** Minimum score for similarity metric */
      min_score?: number;
      /** Fields to include in the search results, empty array/list means all fields. */
      select_fields?: string[];
      /** Include vectors in the search results */
      include_vector?: boolean;
      /** Include the total count of results in the search results */
      include_count?: boolean;
      /** Whether to sort results by ascending or descending order */
      asc?: boolean;
      /** Whether to store the history into VecDB. This will increase the storage costs over time. */
      keep_search_history?: boolean;
      /** Whether to scale up the metric by 100 */
      hundred_scale?: boolean;
      /** The field to cluster on. */
      cluster_vector_field: string;
      /** Number of clusters to be specified. */
      n_clusters: number;
      /** Number of iterations in each run */
      n_iter?: number;
      /** Number of runs to run with different centroid seeds */
      n_init?: number;
      /** Search history ID, only used for storing search histories. */
      search_history_id?: string;
      /** whether to return results in clusters or as a list */
      return_as_clusters?: boolean;
    };
    /** Base class for all abstractmodels */
    AdvancedHybridSearchBody: {
      /** Unique name of dataset */
      dataset_id: string;
      /** Query for advance search that allows for multiple vector and field querying. */
      multivector_query: components["schemas"]["VectorQuery"][];
      /** Positive document IDs to personalize the results with, this will retrive the vectors from the document IDs and consider it in the operation. */
      positive_document_ids?: { [key: string]: unknown };
      /** Negative document IDs to personalize the results with, this will retrive the vectors from the document IDs and consider it in the operation. */
      negative_document_ids?: { [key: string]: unknown };
      /** Aggregation for the vectors when using positive and negative document IDs, choose from ['mean', 'sum', 'min', 'max', 'divide', 'mulitple'] */
      vector_operation?: string;
      /** Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate. */
      approximation_depth?: number;
      /** Whether to sum the multiple vectors similarity search score as 1 or seperate */
      sum_fields?: boolean;
      /** Size of each page of results */
      page_size?: number;
      /** Page of the results */
      page?: number;
      /** Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp'] */
      similarity_metric?: string;
      /** Fields to include in the facets, if [] then all */
      facets?: string[];
      /** Query for filtering the search results */
      filters?: { [key: string]: unknown }[];
      /** Minimum score for similarity metric */
      min_score?: number;
      /** Fields to include in the search results, empty array/list means all fields. */
      select_fields?: string[];
      /** Include vectors in the search results */
      include_vector?: boolean;
      /** Include the total count of results in the search results */
      include_count?: boolean;
      /** Whether to sort results by ascending or descending order */
      asc?: boolean;
      /** Whether to store the history into VecDB. This will increase the storage costs over time. */
      keep_search_history?: boolean;
      /** Whether to scale up the metric by 100 */
      hundred_scale?: boolean;
      /** Search history ID, only used for storing search histories. */
      search_history_id?: string;
      /** Text Search Query (not encoded as vector) */
      text: string;
      /** Text fields to search against */
      fields: string[];
      /** This refers to the amount of letters it takes to reach from 1 string to another string. e.g. band vs bant is a 1 word edit distance. Use -1 if you would like this to be automated. */
      edit_distance?: number;
      /** Whether to consider cases when there is a space in the word. E.g. Go Pro vs GoPro. */
      ignore_spaces?: boolean;
      /** Multiplier of traditional search score. A value of 0.025~0.075 is the ideal range. */
      traditional_weight?: number;
    };
    /** Model for advanced chunk search */
    AdvancedMultiStepChunkSearch: {
      /** Dataset IDs */
      dataset_ids: unknown[];
      /** First step query. */
      first_step_query: components["schemas"]["VectorQuery"][];
      /** Text Search Query (not encoded as vector) */
      first_step_text: string;
      /** Text fields to search against */
      first_step_fields: string[];
      /** This refers to the amount of letters it takes to reach from 1 string to another string. e.g. band vs bant is a 1 word edit distance. Use -1 if you would like this to be automated. */
      first_step_edit_distance?: number;
      /** Whether to consider cases when there is a space in the word. E.g. Go Pro vs GoPro. */
      first_step_ignore_spaces?: boolean;
      /** Multiplier of traditional search score. A value of 0.025~0.075 is the ideal range. */
      first_step_traditional_weight?: number;
      /** Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate. */
      first_step_approximation_depth?: number;
      /** Whether to sum the multiple vectors similarity search score as 1 or seperate */
      first_step_sum_fields?: boolean;
      /** Query for filtering the search results */
      first_step_filters?: unknown[];
      /** In the first search, you are more interested in the contents */
      first_step_page_size?: number;
      /** Advanced chunk query */
      chunk_search_query: unknown[];
      /** Include the total count of results in the search results */
      include_Count?: boolean;
      /** Minimum score for similarity metric */
      min_score?: number;
      /** Size of each page of results */
      page_size?: number;
      /** Include vectors in the search results */
      include_vector?: boolean;
      /** Fields to include in the search results, empty array/list means all fields. */
      select_fields?: unknown[];
    };
    /** Base class for all abstractmodels */
    AdvancedMultiStepChunkSearchQueryBody: {
      /** Unique name of dataset */
      dataset_id: string;
      /** Query for advance search that allows for multiple vector and field querying. */
      multivector_query: components["schemas"]["VectorQuery"][];
      /** Field where the array of chunked documents are. */
      chunk_field?: string;
      /** Scoring method for determining for ranking between document chunks. */
      chunk_scoring?: string;
      /** Size of each page of chunk results */
      chunk_page_size?: number;
      /** Page of the chunk results */
      chunk_page?: number;
      /** Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate. */
      approximation_depth?: number;
      /** Whether to sum the multiple vectors similarity search score as 1 or seperate */
      sum_fields?: boolean;
      /** Size of each page of results */
      page_size?: number;
      /** Page of the results */
      page?: number;
      /** Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp'] */
      similarity_metric?: string;
      /** Fields to include in the facets, if [] then all */
      facets?: unknown[];
      /** Query for filtering the search results */
      filters?: unknown[];
      /** Minimum score for similarity metric */
      min_score?: number;
      /** Include vectors in the search results */
      include_vector?: boolean;
      /** Include the total count of results in the search results */
      include_count?: boolean;
      /** Whether to sort results by ascending or descending order */
      asc?: boolean;
      /** Whether to store the history into VecDB. This will increase the storage costs over time. */
      keep_search_history?: boolean;
      /** Whether to scale up the metric by 100 */
      hundred_scale?: boolean;
      /** Query for advance search that allows for multiple vector and field querying. */
      first_step_multivector_query: components["schemas"]["VectorQuery"][];
      /** Page of the results */
      first_step_page?: number;
      /** Size of each page of results */
      first_step_page_size?: number;
    };
    /** Base class for all abstractmodels */
    AdvancedRecommendBody: {
      /** Unique name of dataset */
      dataset_id: string;
      /** Positive document IDs to personalize the results with, this will retrive the vectors from the document IDs and consider it in the operation. */
      positive_document_ids?: Partial<{ [key: string]: number }> &
        Partial<string[]>;
      /** Negative document IDs to personalize the results with, this will retrive the vectors from the document IDs and consider it in the operation. */
      negative_document_ids?: Partial<{ [key: string]: number }> &
        Partial<string[]>;
      /** The vector field to search in. It can either be an array of strings (automatically equally weighted) (e.g. ['check_vector_', 'yellow_vector_']) or it is a dictionary mapping field to float where the weighting is explicitly specified (e.g. {'check_vector_': 0.2, 'yellow_vector_': 0.5}) */
      vector_fields: Partial<unknown[]> & Partial<{ [key: string]: number }>;
      /** Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate. */
      approximation_depth?: number;
      /** Aggregation for the vectors when using positive and negative document IDs, choose from ['mean', 'sum', 'min', 'max', 'divide', 'mulitple'] */
      vector_operation?: string;
      /** Whether to sum the multiple vectors similarity search score as 1 or seperate */
      sum_fields?: boolean;
      /** Size of each page of results */
      page_size?: number;
      /** Page of the results */
      page?: number;
      /** Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp'] */
      similarity_metric?: string;
      /** Fields to include in the facets, if [] then all */
      facets?: string[];
      /** Query for filtering the search results */
      filters?: { [key: string]: unknown }[];
      /** Minimum score for similarity metric */
      min_score?: number;
      /** Fields to include in the search results, empty array/list means all fields. */
      select_fields?: string[];
      /** Include vectors in the search results */
      include_vector?: boolean;
      /** Include the total count of results in the search results */
      include_count?: boolean;
      /** Whether to sort results by ascending or descending order */
      asc?: boolean;
      /** Whether to store the history into VecDB. This will increase the storage costs over time. */
      keep_search_history?: boolean;
      /** Whether to scale up the metric by 100 */
      hundred_scale?: boolean;
    };
    /** Base class for all abstractmodels */
    AdvancedSearchQueryBody: {
      /** Unique name of dataset */
      dataset_id: string;
      /** Query for advance search that allows for multiple vector and field querying. */
      multivector_query: components["schemas"]["VectorQuery"][];
      /** Positive document IDs to personalize the results with, this will retrive the vectors from the document IDs and consider it in the operation. */
      positive_document_ids?: { [key: string]: unknown };
      /** Negative document IDs to personalize the results with, this will retrive the vectors from the document IDs and consider it in the operation. */
      negative_document_ids?: { [key: string]: unknown };
      /** Aggregation for the vectors when using positive and negative document IDs, choose from ['mean', 'sum', 'min', 'max', 'divide', 'mulitple'] */
      vector_operation?: string;
      /** Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate. */
      approximation_depth?: number;
      /** Whether to sum the multiple vectors similarity search score as 1 or seperate */
      sum_fields?: boolean;
      /** Size of each page of results */
      page_size?: number;
      /** Page of the results */
      page?: number;
      /** Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp'] */
      similarity_metric?: string;
      /** Fields to include in the facets, if [] then all */
      facets?: string[];
      /** Query for filtering the search results */
      filters?: { [key: string]: unknown }[];
      /** Minimum score for similarity metric */
      min_score?: number;
      /** Fields to include in the search results, empty array/list means all fields. */
      select_fields?: string[];
      /** Include vectors in the search results */
      include_vector?: boolean;
      /** Include the total count of results in the search results */
      include_count?: boolean;
      /** Whether to sort results by ascending or descending order */
      asc?: boolean;
      /** Whether to store the history into VecDB. This will increase the storage costs over time. */
      keep_search_history?: boolean;
      /** Whether to scale up the metric by 100 */
      hundred_scale?: boolean;
      /** Search history ID, only used for storing search histories. */
      search_history_id?: string;
    };
    /** Base class for all abstractmodels */
    AggregateLogQuery: {
      /** Query for filtering the search results */
      filters?: unknown[];
      /** Size of each page of results */
      page_size?: number;
      /** Page of the results */
      page?: number;
      /** Whether to sort results by ascending or descending order */
      asc?: boolean;
      flatten?: boolean;
      /** The log dataset IDs to aggregate with - one or more of logs, logs-write, logs-search, logs-task or js-logs */
      log_ids?: Partial<string[]> & Partial<string>;
    };
    /** Base class for all abstractmodels */
    AggregateQueryV2Commons: {
      /** Unique name of dataset */
      dataset_ids: Partial<string[]> & Partial<string>;
      /** Aggregation query to aggregate data */
      aggregation_query: { [key: string]: unknown };
      /** Query for filtering the search results */
      filters?: unknown[];
      /** Size of each page of results */
      page_size?: number;
      /** Page of the results */
      page?: number;
      /** Whether to sort results by ascending or descending order */
      asc?: boolean;
      flatten?: boolean;
      /** Alias used to name a vector field. Belongs in field_{alias}_vector_ */
      alias?: string;
    };
    /** Base class for all abstractmodels */
    BulkDeleteDocuments: {
      /** IDs of documents */
      ids: string[];
    };
    /** Base class for each response model */
    BulkDeleteResponse: {
      deleted: string[];
      not_found: string[];
      status: string;
    };
    /** Base Class for POST models. */
    BulkDocProcessing: {
      task_name?: string;
      /** A list of PDF file urls. */
      file_urls?: string[];
      /** A list of PDF filenames. */
      filenames?: string[];
    };
    /** Bulk Encode Documents with Deep Learning Models */
    BulkEncodeModel: {
      /** A list of documents. Document is a JSON-like data that we store our metadata and vectors with. For specifying id of the document use the field '\_id', for specifying vector field use the suffix of '\_vector\_' */
      documents?: string;
      /**
       * An array structure of models to encode fields with.
       *
       *     [
       *         {"model_url": "https://a_vector_model_url.com/encode_image_url", "body": "url", "field": "thumbnail"},
       *         {"model_url": "https://a_vector_model_url.com/encode_text", "body": "text", "field": "short_description"},
       *         {"model_url": "bert", "body": "text", "field": "short_description", "alias":"bert"},
       *     ]
       */
      encoders: { [key: string]: unknown }[];
      /** Whether to return original document, or not */
      include_document?: boolean;
    };
    /** Base class for all abstractmodels */
    BulkEncodeResponse: {
      documents: { [key: string]: unknown }[];
      error: string;
      message: string;
    };
    /** Base class for all abstractmodels */
    BulkGetDocuments: {
      /** IDs of documents */
      ids: string[];
      /** Fields to include in the search results, empty array/list means all fields. */
      select_fields?: string[];
      /** Include vectors in the search results */
      include_vector?: boolean;
    };
    /** Base class for all abstractmodels */
    BulkInsertBody: {
      /** A list of documents. Document is a JSON-like data that we store our metadata and vectors with. For specifying id of the document use the field '\_id', for specifying vector field use the suffix of '\_vector\_' */
      documents?: { [key: string]: unknown }[];
      /** Whether to include insert date as a field 'insert_date_'. */
      insert_date?: boolean;
      /** Whether to overwrite document if it exists. */
      overwrite?: boolean;
      /** Whether the api should check the documents for vector datatype to update the schema. */
      update_schema?: boolean;
      /** Include the inserted IDs in the response */
      include_inserted_ids?: boolean;
    };
    /** Return an inserted and what failed. */
    BulkInsertResponse: {
      documents_received: number;
      inserted: number;
      inserted_ids?: string[];
      failed_documents: { [key: string]: unknown }[];
    };
    /** Bulk Process PDFs */
    BulkPDFProcessing: {
      task_name?: string;
      /** A list of PDF file urls. */
      file_urls?: string[];
      /** A list of PDF filenames. */
      filenames?: string[];
    };
    /** Base class for all abstractmodels */
    BulkUpdateBody: {
      /** Updates to make to the documents. It should be specified in a format of {"field_name": "value"}. e.g. {"item.status" : "Sold Out"} */
      updates: { [key: string]: unknown }[];
      /** Whether to include insert date as a field 'insert_date_'. */
      insert_date?: boolean;
      /** Include the inserted IDs in the response */
      include_updated_ids?: boolean;
    };
    /** Return an updated and what failed. */
    BulkUpdateResponse: {
      documents_received: number;
      updated: number;
      updated_ids?: string[];
      failed_documents: { [key: string]: unknown }[];
    };
    /**
     * Within a collection encode the specified categories field in every document into vectors.
     *
     * For example, categories that represents a ****movie's categories, field "movie_categories"**:
     *
     *     document 1 categories field: {"category" : ["sci-fi", "thriller", "comedy"]}
     *
     *     document 2 categories field: {"category" : ["sci-fi", "romance", "drama"]}
     *
     *     -> <Encode the categories to vectors> ->
     *
     * | sci-fi | thriller | comedy | romance | drama |
     * |--------|----------|--------|---------|-------|
     * | 1      | 1        | 1      | 0       | 0     |
     * | 1      | 0        | 0      | 1       | 1     |
     *
     *     document 1 categories vector: {"movie_categories_vector_": [1, 1, 1, 0, 0]}
     *
     *     document 2 categories vector: {"movie_categories_vector_": [1, 0, 0, 1, 1]}
     */
    CategoriesEncoder: {
      /** Encode cateogires/array field to a vector. */
      task_name?: string;
      /** The categories/array field to train on to encode into vectors */
      fields: string[];
    };
    /** Encode a categories model */
    CategoriesModel: {
      /** Unique name of dataset */
      dataset_id: string;
      /** The array/categories field that encoding is trained on */
      field: string;
      /** The array of categories to encode into vectors */
      categories: string[];
    };
    /** Centroid documents */
    CentroidDocuments: {
      /** Unique name of dataset */
      dataset_id: string;
      /** List of cluster IDs */
      cluster_ids: string[];
      /** The vector field where a clustering task was run. */
      vector_field: string;
      /** Alias is used to name a cluster */
      alias?: string;
      /** Size of each page of results */
      page_size?: number;
      /** Cursor to paginate the document retrieval */
      cursor?: string;
      /** Page of the results */
      page?: number;
      /** Include vectors in the search results */
      include_vector?: boolean;
      /** Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp'] */
      similarity_metric?: string;
    };
    /** Adds support for multi vector field clustering */
    CentroidInsertBodyV2: {
      /** Unique name of dataset */
      dataset_id: string;
      /** Cluster centers with the key being the index number */
      cluster_centers: { [key: string]: unknown }[];
      /** The vector field to search in. It can either be an array of strings (automatically equally weighted) (e.g. ['check_vector_', 'yellow_vector_']) or it is a dictionary mapping field to float where the weighting is explicitly specified (e.g. {'check_vector_': 0.2, 'yellow_vector_': 0.5}) */
      vector_fields: unknown[];
      /** Include the inserted IDs in the response */
      include_inserted_ids?: boolean;
      /** Alias used to name a vector field. Belongs in field_{alias}_vector_ */
      alias?: string;
    };
    /** Base class for all abstractmodels */
    CentroidMetadataV2: {
      /** Unique name of dataset */
      dataset_id: string;
      /** Description for a metadata */
      metadata?: { [key: string]: unknown };
      /** The vector field to search in. It can either be an array of strings (automatically equally weighted) (e.g. ['check_vector_', 'yellow_vector_']) or it is a dictionary mapping field to float where the weighting is explicitly specified (e.g. {'check_vector_': 0.2, 'yellow_vector_': 0.5}) */
      vector_fields: unknown[];
      /** Alias used to name a vector field. Belongs in field_{alias}_vector_ */
      alias?: string;
    };
    /** Base class for all abstractmodels */
    CentroidUpdateBodyV2: {
      /** Unique name of dataset */
      dataset_id: string;
      /** The vector field to search in. It can either be an array of strings (automatically equally weighted) (e.g. ['check_vector_', 'yellow_vector_']) or it is a dictionary mapping field to float where the weighting is explicitly specified (e.g. {'check_vector_': 0.2, 'yellow_vector_': 0.5}) */
      vector_fields: unknown[];
      /** Alias used to name a vector field. Belongs in field_{alias}_vector_ */
      alias?: string;
      /** ID of a centroid in a dataset. */
      id: string;
      /** A dictionary to edit and add fields to a document. */
      update: { [key: string]: unknown };
    };
    /** Base class for all abstractmodels */
    CentroidsClosestToCenterV2: {
      /** Unique name of dataset */
      dataset_id: string;
      /** The vector field to search in. It can either be an array of strings (automatically equally weighted) (e.g. ['check_vector_', 'yellow_vector_']) or it is a dictionary mapping field to float where the weighting is explicitly specified (e.g. {'check_vector_': 0.2, 'yellow_vector_': 0.5}) */
      vector_fields: unknown[];
      /** Alias used to name a vector field. Belongs in field_{alias}_vector_ */
      alias?: string;
      /** List of cluster IDs */
      cluster_ids?: string[];
      /** Centroid vector field */
      centroid_vector_field?: string[];
      /** Fields to include in the search results, empty array/list means all fields. */
      select_fields?: string[];
      /** Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate. */
      approx?: number;
      /** Whether to sum the multiple vectors similarity search score as 1 or seperate */
      sum_fields?: boolean;
      /** Size of each page of results */
      page_size?: number;
      /** Page of the results */
      page?: number;
      /** Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp'] */
      similarity_metric?: string;
      /** Query for filtering the search results */
      filters?: unknown[];
      /** Fields to include in the facets, if [] then all */
      facets?: unknown[];
      /** Minimum score for similarity metric */
      min_score?: number;
      /** Include vectors in the search results */
      include_vector?: boolean;
      /** Include the total count of results in the search results */
      include_count?: boolean;
      /** Include facets in the search results */
      include_facets?: boolean;
    };
    /** Base class for all abstractmodels */
    CentroidsFurthestFromCenterV2: {
      /** Unique name of dataset */
      dataset_id: string;
      /** The vector field to search in. It can either be an array of strings (automatically equally weighted) (e.g. ['check_vector_', 'yellow_vector_']) or it is a dictionary mapping field to float where the weighting is explicitly specified (e.g. {'check_vector_': 0.2, 'yellow_vector_': 0.5}) */
      vector_fields: unknown[];
      /** Alias used to name a vector field. Belongs in field_{alias}_vector_ */
      alias?: string;
      /** List of cluster IDs */
      cluster_ids?: string[];
      /** Centroid vector field */
      centroid_vector_field?: string[];
      /** Fields to include in the search results, empty array/list means all fields. */
      select_fields?: string[];
      /** Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate. */
      approx?: number;
      /** Whether to sum the multiple vectors similarity search score as 1 or seperate */
      sum_fields?: boolean;
      /** Size of each page of results */
      page_size?: number;
      /** Page of the results */
      page?: number;
      /** Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp'] */
      similarity_metric?: string;
      /** Query for filtering the search results */
      filters?: unknown[];
      /** Fields to include in the facets, if [] then all */
      facets?: unknown[];
      /** Minimum score for similarity metric */
      min_score?: number;
      /** Include vectors in the search results */
      include_vector?: boolean;
      /** Include the total count of results in the search results */
      include_count?: boolean;
      /** Include facets in the search results */
      include_facets?: boolean;
    };
    /** Base class for all abstractmodels */
    CentroidsGetV2: {
      /** Unique name of dataset */
      dataset_id: string;
      /** List of cluster IDs */
      cluster_ids: string[];
      /** The vector field where a clustering task was run. */
      vector_fields: unknown[];
      /** Alias is used to name a cluster */
      alias?: string;
      /** Size of each page of results */
      page_size?: number;
      /** Cursor to paginate the document retrieval */
      cursor?: string;
      /** Include vectors in the search results */
      include_vector?: boolean;
    };
    /** Base class for all abstractmodels */
    ChunkSearchQuery: {
      /** Unique name of dataset */
      dataset_id: string;
      /** Query for advance search that allows for multiple vector and field querying. */
      multivector_query: components["schemas"]["VectorQuery"][];
      /** Field where the array of chunked documents are. */
      chunk_field?: string;
      /** Scoring method for determining for ranking between document chunks. */
      chunk_scoring?: string;
      /** Size of each page of chunk results */
      chunk_page_size?: number;
      /** Page of the chunk results */
      chunk_page?: number;
      /** Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate. */
      approximation_depth?: number;
      /** Whether to sum the multiple vectors similarity search score as 1 or seperate */
      sum_fields?: boolean;
      /** Size of each page of results */
      page_size?: number;
      /** Page of the results */
      page?: number;
      /** Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp'] */
      similarity_metric?: string;
      /** Fields to include in the facets, if [] then all */
      facets?: unknown[];
      /** Query for filtering the search results */
      filters?: unknown[];
      /** Minimum score for similarity metric */
      min_score?: number;
      /** Include vectors in the search results */
      include_vector?: boolean;
      /** Include the total count of results in the search results */
      include_count?: boolean;
      /** Whether to sort results by ascending or descending order */
      asc?: boolean;
      /** Whether to store the history into VecDB. This will increase the storage costs over time. */
      keep_search_history?: boolean;
      /** Whether to scale up the metric by 100 */
      hundred_scale?: boolean;
    };
    /** Base class for each response model */
    ChunkSearchResponse: {
      results: { [key: string]: unknown }[];
      count?: number;
    };
    /** Base class for all abstractmodels */
    ClusterAggregateQueryCommonsV2: {
      /** Unique name of dataset */
      dataset_id: string;
      /** Aggregation query to aggregate data */
      aggregation_query: { [key: string]: unknown };
      /** Query for filtering the search results */
      filters?: unknown[];
      /** Size of each page of results */
      page_size?: number;
      /** Page of the results */
      page?: number;
      /** Whether to sort results by ascending or descending order */
      asc?: boolean;
      flatten?: boolean;
      /** Alias used to name a vector field. Belongs in field_{alias}_vector_ */
      alias?: string;
      /** The vector field to search in. It can either be an array of strings (automatically equally weighted) (e.g. ['check_vector_', 'yellow_vector_']) or it is a dictionary mapping field to float where the weighting is explicitly specified (e.g. {'check_vector_': 0.2, 'yellow_vector_': 0.5}) */
      vector_fields: string[];
    };
    /** Base class for all abstractmodels */
    ClustercentroidsList: {
      /** Unique name of dataset */
      dataset_id: string;
      /** The vector field where a clustering task was run. */
      vector_fields: unknown[];
      /** Alias is used to name a cluster */
      alias?: string;
      /** Size of each page of results */
      page_size?: number;
      /** Cursor to paginate the document retrieval */
      cursor?: string;
      /** Include vectors in the search results */
      include_vector?: boolean;
    };
    /** Cluster Task */
    Clusterer: {
      task_name?: string;
      /** The field to cluster on. */
      vector_field: string;
      /** Alias is used to name a cluster */
      alias?: string;
      /** Number of clusters to be specified. */
      n_clusters: number;
      /** Number of iterations in each run */
      n_iter?: number;
      /** Number of runs to run with different centroid seeds */
      n_init?: number;
      /** Whether to rerun task on the whole dataset or just the ones missing the output */
      refresh?: boolean;
    };
    /** Base class for all abstractmodels */
    CopyCollectionBody: {
      /** Unique name of dataset */
      new_dataset_id: string;
      /** Schema for specifying the field that are vectors and its length */
      schema?: { [key: string]: unknown };
      /** Fields to rename {'old_field': 'new_field'}. Defaults to no renames */
      rename_fields?: { [key: string]: unknown };
      /** Fields to remove ['random_field', 'another_random_field']. Defaults to no removes */
      remove_fields?: string[];
      /** Query for filtering the search results */
      filters?: { [key: string]: unknown }[];
    };
    /** Base class for all abstractmodels */
    CopyForeignDatasetBody: {
      /** Dataset name to copy into */
      dataset_id: string;
      /** project name you want to copy the dataset into */
      project: string;
      /** api key of the project you want to copy the dataset into */
      api_key: string;
      /** Dataset to copy frpm */
      source_dataset_id: string;
      /** Source project name of whom the dataset belongs to */
      source_project: string;
      /** Api key to access the source project name */
      source_api_key: string;
      /** Query for filtering the dataset */
      filters?: { [key: string]: unknown }[];
    };
    /** Base class for all abstractmodels */
    CreateDatasetBody: {
      /** Unique name of dataset */
      id: string;
      /** Schema for specifying the field that are vectors and its length */
      schema?: { [key: string]: unknown };
    };
    /** Base class for all abstractmodels */
    CreateDatasetResponse: {
      status: string;
      message: string;
    };
    /** Base class for all abstractmodels */
    CreateDeployableBody: {
      /** The deployable configuration. */
      configuration?: { [key: string]: unknown };
      /** Unique name of dataset, */
      dataset_id?: string;
    };
    /** Base class for each response model */
    CreateDeployableResponse: {
      deployable_id: string;
      dataset_id?: string;
      project_id?: string;
      api_key?: string;
      configuration?: { [key: string]: unknown };
    };
    /** Base class for all abstractmodels */
    DatasetDeleteBody: {
      /** Unique name of dataset */
      dataset_id: string;
    };
    /** Base class for all abstractmodels */
    DeleteDeployableBody: {
      /** The deployable ID. */
      id: string;
    };
    /** Base class for each response model */
    DeleteDeployableResponse: {
      status: string;
      message: string;
    };
    /** Base class for all abstractmodels */
    DeleteDocumentBody: {
      /** ID of a document in a dataset. */
      id: string;
    };
    /** Base class for all abstractmodels */
    DeleteFieldsBody: {
      /** ID of a document in a dataset. */
      id: string;
      /** List of fields to delete in a document */
      fields: string[];
    };
    /** Base class for all abstractmodels */
    DeleteWhereBody: {
      /** Query for filtering the search results */
      filters?: { [key: string]: unknown }[];
    };
    /**
     * Within a collection encode the specified dictionary field in every document into vectors.
     *
     * For example: a dictionary that represents a **person's characteristics visiting a store, field "person_characteristics"**:
     *
     *     document 1 field: {"person_characteristics" : {"height":180, "age":40, "weight":70}}
     *
     *     document 2 field: {"person_characteristics" : {"age":32, "purchases":10, "visits": 24}}
     *
     *     -> <Encode the dictionaries to vectors> ->
     *
     * | height | age | weight | purchases | visits |
     * |--------|-----|--------|-----------|--------|
     * | 180    | 40  | 70     | 0         | 0      |
     * | 0      | 32  | 0      | 10        | 24     |
     *
     *     document 1 dictionary vector: {"person_characteristics_vector_": [180, 40, 70, 0, 0]}
     *
     *     document 2 dictionary vector: {"person_characteristics_vector_": [0, 32, 0, 10, 24]}
     */
    DictionaryEncoder: {
      /** Encode dictionary field to a vector. */
      task_name?: string;
      /** The dictionary field to train on to encode into vectors. */
      fields: string[];
    };
    /** Encode a dictionary model */
    DictionaryModel: {
      /** Unique name of dataset */
      dataset_id: string;
      /** The dictionary field that encoding is trained on */
      field: string;
      /** The dictionary to encode into vectors */
      dictionary: { [key: string]: unknown };
    };
    /** Base class for all abstractmodels */
    DiversityModel: {
      /** Unique name of dataset */
      dataset_id: string;
      /** Query for advance search that allows for multiple vector and field querying. */
      multivector_query: components["schemas"]["VectorQuery"][];
      /** Positive document IDs to personalize the results with, this will retrive the vectors from the document IDs and consider it in the operation. */
      positive_document_ids?: { [key: string]: unknown };
      /** Negative document IDs to personalize the results with, this will retrive the vectors from the document IDs and consider it in the operation. */
      negative_document_ids?: { [key: string]: unknown };
      /** Aggregation for the vectors when using positive and negative document IDs, choose from ['mean', 'sum', 'min', 'max', 'divide', 'mulitple'] */
      vector_operation?: string;
      /** Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate. */
      approximation_depth?: number;
      /** Whether to sum the multiple vectors similarity search score as 1 or seperate */
      sum_fields?: boolean;
      /** Size of each page of results */
      page_size?: number;
      /** Page of the results */
      page?: number;
      /** Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp'] */
      similarity_metric?: string;
      /** Fields to include in the facets, if [] then all */
      facets?: string[];
      /** Query for filtering the search results */
      filters?: { [key: string]: unknown }[];
      /** Minimum score for similarity metric */
      min_score?: number;
      /** Fields to include in the search results, empty array/list means all fields. */
      select_fields?: string[];
      /** Include vectors in the search results */
      include_vector?: boolean;
      /** Include the total count of results in the search results */
      include_count?: boolean;
      /** Whether to sort results by ascending or descending order */
      asc?: boolean;
      /** Whether to store the history into VecDB. This will increase the storage costs over time. */
      keep_search_history?: boolean;
      /** Whether to scale up the metric by 100 */
      hundred_scale?: boolean;
      /** Search history ID, only used for storing search histories. */
      search_history_id?: string;
      /** The field to cluster on. */
      cluster_vector_field: string;
      /** Number of clusters to be specified. */
      n_clusters: number;
      /** Number of runs to run with different centroid seeds */
      n_init?: number;
      /** Number of iterations in each run */
      n_iter?: number;
      /** If True, return as clusters as opposed to results list */
      return_as_clusters?: boolean;
    };
    /** Cluster and then tag the body */
    DiversityTag: {
      /** Image Url or text or any data suited for the encoder */
      data: string;
      /** Name of the dataset you want to tag */
      tag_dataset_id: string;
      /** Which encoder to use. */
      encoder: { [key: string]: unknown };
      /** The field used to tag in a dataset. If None, automatically uses the one stated in the encoder. */
      tag_field?: string;
      /** Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate. */
      approximation_depth?: number;
      /** Whether to sum the multiple vectors similarity search score as 1 or seperate */
      sum_fields?: boolean;
      /** Size of each page of results */
      page_size?: number;
      /** Page of the results */
      page?: number;
      /** Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp'] */
      similarity_metric?: string;
      /** Query for filtering the search results */
      filters?: unknown[];
      /** Minimum score for similarity metric */
      min_score?: number;
      /** Whether to calculate a search_relevance cutoff score to flag relevant and less relevant results */
      include_search_relevance?: boolean;
      /** How aggressive the search_relevance cutoff score is (higher value the less results will be relevant) */
      search_relevance_cutoff_aggressiveness?: number;
      /** Whether to sort results by ascending or descending order */
      asc?: boolean;
      /** Whether to include score */
      include_score?: boolean;
      /** The field to cluster on. */
      cluster_vector_field: string;
      /** Number of clusters to be specified. */
      n_clusters: number;
      /** Number of iterations in each run */
      n_iter?: number;
      /** Number of runs to run with different centroid seeds */
      n_init?: number;
    };
    /** Base class for all abstractmodels */
    DocumentDiffBody: {
      /** Main document to compare other documents against. */
      doc: { [key: string]: unknown };
      /** Other documents to compare against the main document. */
      docs_to_compare: { [key: string]: unknown }[];
      /** Fields to compare. Defaults to [], which compares all fields. */
      difference_fields?: unknown[];
    };
    /** Base class for each response model */
    DocumentDiffResponse: {
      differences: unknown[];
    };
    /** Base class for all abstractmodels */
    DocumentResponse: {
      /** A list of documents. Document is a JSON-like data that we store our metadata and vectors with. For specifying id of the document use the field '\_id', for specifying vector field use the suffix of '\_vector\_' */
      document?: { [key: string]: unknown };
    };
    /** Base class for all abstractmodels */
    DocumentsResponse: {
      /** A list of documents. Document is a JSON-like data that we store our metadata and vectors with. For specifying id of the document use the field '\_id', for specifying vector field use the suffix of '\_vector\_' */
      documents?: Partial<{ [key: string]: unknown }> &
        Partial<{ [key: string]: unknown }[]>;
    };
    /** Base class for all abstractmodels */
    EncodeImageModel: {
      image: Partial<string> & Partial<string>;
    };
    /** Encode a Document with Deep Learning Models */
    EncodeModel: {
      /** A list of documents. Document is a JSON-like data that we store our metadata and vectors with. For specifying id of the document use the field '\_id', for specifying vector field use the suffix of '\_vector\_' */
      document?: string;
      /**
       * An array structure of models to encode fields with.
       *
       *     [
       *         {"model_url": "https://a_vector_model_url.com/encode_image_url", "body": "url", "field": "thumbnail"},
       *         {"model_url": "https://a_vector_model_url.com/encode_text", "body": "text", "field": "short_description"},
       *         {"model_url": "bert", "body": "text", "field": "short_description", "alias":"bert"},
       *     ]
       */
      encoders: { [key: string]: unknown }[];
      /** Whether to return original document, or not */
      include_document?: boolean;
    };
    /** Base class for all abstractmodels */
    EncodeResponse: {
      document: { [key: string]: unknown };
      error: string;
      message: string;
    };
    /** Base class for all abstractmodels */
    FacetsBody: {
      /** Fields to include in the facets, if [] then all */
      fields?: string[];
      /** Interval for date facets */
      date_interval?: string;
      /** Size of facet page */
      page_size?: number;
      /** Page of the results */
      page?: number;
      /** Whether to sort results by ascending or descending order */
      asc?: boolean;
    };
    /** Base class for each response model */
    GetDeployableResponse: {
      project_id?: string;
      dataset_id?: string;
      api_key?: string;
      configuration?: { [key: string]: unknown };
    };
    /** Base class for all abstractmodels */
    GetWhereDocuments: {
      /** Fields to include in the search results, empty array/list means all fields. */
      select_fields?: string[];
      /** Cursor to paginate the document retrieval */
      cursor?: string;
      /** Size of each page of results */
      page_size?: number;
      /** Fields to sort by. For each field, sort by descending or ascending. If you are using descending by datetime, it will get the most recent ones. */
      sort?: unknown[];
      /** Include vectors in the search results */
      include_vector?: boolean;
      /** Query for filtering the search results */
      filters?: unknown[];
      /** If True, retrieves doucments randomly. Cannot be used with cursor. */
      is_random?: boolean;
      /** Random Seed for retrieving random documents. */
      random_state?: number;
    };
    HTTPValidationError: {
      detail?: components["schemas"]["ValidationError"][];
    };
    /** Base Class for POST models. */
    ImageTextEncoder: {
      /** The task name. */
      task_name?: string;
      /** Unique name of dataset */
      dataset_id: string;
      /** The field to encode */
      field: string;
      /** Alias used to name a vector field. Belongs in field_{alias}_vector_ */
      alias?: string;
      /** Whether to rerun task on the whole dataset or just the ones missing the output */
      refresh?: boolean;
    };
    /** Base class for all abstractmodels */
    InferSchemaBody: {
      /** Unique name of dataset */
      id: string;
      /** A list of documents. Document is a JSON-like data that we store our metadata and vectors with. For specifying id of the document use the field '\_id', for specifying vector field use the suffix of '\_vector\_' */
      document?: { [key: string]: unknown };
    };
    /** Base class for all abstractmodels */
    InsertBody: {
      /** A list of documents. Document is a JSON-like data that we store our metadata and vectors with. For specifying id of the document use the field '\_id', for specifying vector field use the suffix of '\_vector\_' */
      document?: { [key: string]: unknown };
      /** Whether to include insert date as a field 'insert_date_'. */
      insert_date?: boolean;
      /** Whether to overwrite document if it exists. */
      overwrite?: boolean;
      /** Whether the api should check the documents for vector datatype to update the schema. */
      update_schema?: boolean;
    };
    /** Base class for each response model */
    InsertDocumentResponse: {
      status: string;
      message: string;
    };
    /** Base class for all abstractmodels */
    ListDatasetsBody: {
      /** Whether to return schema */
      include_schema?: boolean;
      /** Whether to return stats */
      include_stats?: boolean;
      /** Whether to return metadata */
      include_metadata?: boolean;
      /** Whether to return schema stats */
      include_schema_stats?: boolean;
      /** Whether to return vector health */
      include_vector_health?: boolean;
      /** Whether to return active jobs */
      include_active_jobs?: boolean;
      /** List of dataset IDs */
      dataset_ids?: string[];
      /** Sort by created at date. By default shows the newest datasets. Set asc=False to get oldest dataset. */
      sort_by_created_at_date?: boolean;
      /** Whether to sort results by ascending or descending order */
      asc?: boolean;
      /** Size of each page of results */
      page_size?: number;
      /** Page of the results */
      page?: number;
    };
    /** Base class for each response model */
    ListDeployableResponse: {
      deployables: { [key: string]: unknown }[];
      count: number;
    };
    /** Base class for all abstractmodels */
    MetadataPostBody: {
      /** Unique name of dataset */
      dataset_id: string;
      /** Metadata for a dataset, e.g. {'description': 'dataset for searching products'} */
      metadata: { [key: string]: unknown };
    };
    /**
     * Within a collection encode the specified fields in every document into vectors.
     *
     * For example: we choose the fields ["height", "age", "weight"]
     *     document 1 field: {"height":180, "age":40, "weight":70, "purchases":20, "visits": 12}
     *
     *     document 2 field: {"height":160, "age":32, "weight":50, "purchases":10, "visits": 24}
     *
     *     -> <Encode the fields to vectors> ->
     *
     * | height | age | weight |
     * |--------|-----|--------|
     * | 180    | 40  | 70     |
     * | 160    | 32  | 50     |
     *
     *     document 1 vector: {"person_characteristics_vector_": [180, 40, 70]}
     *
     *     document 2 vector: {"person_characteristics_vector_": [160, 32, 50]}
     */
    NumericFieldsEncoder: {
      /** Encode Numeric fields to a vector. */
      task_name?: string;
      /** The numeric fields to encode into vectors. */
      fields: string[];
      /** The name of the vector field created. */
      vector_name?: string;
    };
    /** Encode numeric fields to vec model */
    NumericFieldsModel: {
      /** Unique name of dataset */
      dataset_id: string;
      /** A list of documents. Document is a JSON-like data that we store our metadata and vectors with. For specifying id of the document use the field '\_id', for specifying vector field use the suffix of '\_vector\_' */
      document?: { [key: string]: unknown };
      /** The name of the created vector. */
      vector_name: string;
    };
    /** Base class for all abstractmodels */
    PaginateDocuments: {
      /** Fields to include in the search results, empty array/list means all fields. */
      select_fields?: string[];
      /** Page of the results */
      page?: number;
      /** Size of each page of results */
      page_size?: number;
      /** Fields to sort by. For each field, sort by descending or ascending. If you are using descending by datetime, it will get the most recent ones. */
      sort?: unknown[];
      /** Include vectors in the search results */
      include_vector?: boolean;
      /** Query for filtering the search results */
      filters?: unknown[];
    };
    /** Base class for all abstractmodels */
    PostVectorMappings: {
      /** Array mappings */
      array_mappings: { [key: string]: unknown };
      /** Mapping ID */
      mapping_id: string;
    };
    /** Base class for all abstractmodels */
    PredictKNNClassificationFromResultsModel: {
      /** Field in results to use for the prediction. Can be multiplied with weighting. */
      field: string;
      /** List of results in a dictionary */
      results: { [key: string]: unknown }[];
      /** The weighting for each prediction */
      weighting?: Partial<boolean> & Partial<number[]>;
      /** The value used to fill in the document when the data is missing. */
      impute_value?: number;
      /** How to predict using the vectors. One of `most_frequent` or `sum_scores */
      prediction_operation?: string;
    };
    /** This is the body of argumetns required for tagging. */
    PredictKNNregressionModel: {
      /** Unique name of dataset */
      dataset_id: string;
      /** Vector, a list/array of floats that represents a piece of data. */
      vector: number[];
      /** The vector field to search in. It can either be an array of strings (automatically equally weighted) (e.g. ['check_vector_', 'yellow_vector_']) or it is a dictionary mapping field to float where the weighting is explicitly specified (e.g. {'check_vector_': 0.2, 'yellow_vector_': 0.5}) */
      vector_field: string;
      /** The field to perform regression on. */
      target_field: string;
      /** The number of results for KNN. */
      k?: number;
      /** The weighting for each prediction */
      weighting?: Partial<boolean> & Partial<number[]>;
      /** The value used to fill in the document when the data is missing. */
      impute_value?: number;
      /** How to predict using the vectors. One of `most_frequent` or `sum_scores */
      predict_operation?: string;
      /** Whether to include search results. */
      include_search_results?: boolean;
    };
    /** Results of the recommendations */
    RecommendResults: {
      results: { [key: string]: unknown }[];
      count?: number;
      facets?: { [key: string]: unknown }[];
    };
    /** Base class for all abstractmodels */
    RequestReadApiKeyBody: {
      /** Username for read only key */
      read_username: string;
    };
    /** Results of the recommendations */
    SearchResults: {
      results: unknown[];
      count?: number;
      facets?: Partial<{ [key: string]: unknown }> & Partial<unknown[]>;
    };
    /** Base class for all abstractmodels */
    SemanticSearchBody: {
      /** Unique name of dataset */
      dataset_id: string;
      /** Query for advance search that allows for multiple vector and field querying. */
      multivector_query: components["schemas"]["VectorQuery"][];
      /** Text Search Query (not encoded as vector) */
      text: string;
      /** Text fields to search against */
      fields: string[];
      /** Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate. */
      approximation_depth?: number;
      /** Whether to sum the multiple vectors similarity search score as 1 or seperate */
      sum_fields?: boolean;
      /** Size of each page of results */
      page_size?: number;
      /** Page of the results */
      page?: number;
      /** Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp'] */
      similarity_metric?: string;
      /** Fields to include in the facets, if [] then all */
      facets?: string[];
      /** Query for filtering the search results */
      filters?: { [key: string]: unknown }[];
      /** Minimum score for similarity metric */
      min_score?: number;
      /** Fields to include in the search results, empty array/list means all fields. */
      select_fields?: string[];
      /** Include vectors in the search results */
      include_vector?: boolean;
      /** Include the total count of results in the search results */
      include_count?: boolean;
      /** Whether to sort results by ascending or descending order */
      asc?: boolean;
      /** Whether to store the history into VecDB. This will increase the storage costs over time. */
      keep_search_history?: boolean;
      /** Whether to scale up the metric by 100 */
      hundred_scale?: boolean;
    };
    /** Base class for all abstractmodels */
    StoreEncodersPipelineBody: {
      /** encoders */
      encoders: { [key: string]: unknown }[];
    };
    /** This is the body of argumetns required for tagging. */
    TagBody: {
      /** Image Url or text or any data suited for the encoder */
      data: string;
      /** Name of the dataset you want to tag */
      tag_dataset_id: string;
      /** Which encoder to use. */
      encoder: { [key: string]: unknown };
      /** The field used to tag in a dataset. If None, automatically uses the one stated in the encoder. */
      tag_field?: string;
      /** Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate. */
      approximation_depth?: number;
      /** Whether to sum the multiple vectors similarity search score as 1 or seperate */
      sum_fields?: boolean;
      /** Size of each page of results */
      page_size?: number;
      /** Page of the results */
      page?: number;
      /** Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp'] */
      similarity_metric?: string;
      /** Query for filtering the search results */
      filters?: unknown[];
      /** Minimum score for similarity metric */
      min_score?: number;
      /** Whether to calculate a search_relevance cutoff score to flag relevant and less relevant results */
      include_search_relevance?: boolean;
      /** How aggressive the search_relevance cutoff score is (higher value the less results will be relevant) */
      search_relevance_cutoff_aggressiveness?: number;
      /** Whether to sort results by ascending or descending order */
      asc?: boolean;
      /** Whether to include score */
      include_score?: boolean;
    };
    /** Base class for each response model */
    TaskResponse: {
      task_id: string;
    };
    /** Base Class for POST models. */
    TextEncoder: {
      /** The task name. */
      task_name?: string;
      /** Unique name of dataset */
      dataset_id: string;
      /** The field to encode */
      field: string;
      /** Alias used to name a vector field. Belongs in field_{alias}_vector_ */
      alias?: string;
      /** Whether to rerun task on the whole dataset or just the ones missing the output */
      refresh?: boolean;
    };
    /** Base Class for POST models. */
    TextImageEncoder: {
      /** The task name. */
      task_name?: string;
      /** Unique name of dataset */
      dataset_id: string;
      /** The field to encode */
      field: string;
      /** Alias used to name a vector field. Belongs in field_{alias}_vector_ */
      alias?: string;
      /** Whether to rerun task on the whole dataset or just the ones missing the output */
      refresh?: boolean;
    };
    /** Text Multi Encoder */
    TextMultiEncoder: {
      task_name?: string;
      /** Unique name of dataset */
      dataset_id: string;
      /** The field to encode */
      field: string;
      /** Alias used to name a vector field. Belongs in field_{alias}_vector_ */
      alias?: string;
      /** Whether to rerun task on the whole dataset or just the ones missing the output */
      refresh?: boolean;
    };
    /** Base Class for POST models. */
    TextSentiment: {
      task_name?: string;
      /** Unique name of dataset */
      dataset_id: string;
      /** field */
      field: string;
      /** Alias used to name a vector field. Belongs in field_{alias}_vector_ */
      alias?: string;
    };
    /** Base class for all abstractmodels */
    TraditionalSearchBody: {
      /** Unique name of dataset */
      dataset_id: string;
      /** Text Search Query (not encoded as vector) */
      text: string;
      /** Text fields to search against */
      fields: string[];
      /** This refers to the amount of letters it takes to reach from 1 string to another string. e.g. band vs bant is a 1 word edit distance. Use -1 if you would like this to be automated. */
      edit_distance?: number;
      /** Whether to consider cases when there is a space in the word. E.g. Go Pro vs GoPro. */
      ignore_spaces?: boolean;
      /** Size of each page of results */
      page_size?: number;
      /** Page of the results */
      page?: number;
      /** Fields to include in the search results, empty array/list means all fields. */
      select_fields?: string[];
      /** Include vectors in the search results */
      include_vector?: boolean;
      /** Include the total count of results in the search results */
      include_count?: boolean;
      /** Whether to sort results by ascending or descending order */
      asc?: boolean;
      /** Whether to store the history into VecDB. This will increase the storage costs over time. */
      keep_search_history?: boolean;
      /** Search history ID, only used for storing search histories. */
      search_history_id?: string;
    };
    /** Base class for all abstractmodels */
    UpdateBody: {
      /** ID of a document in a dataset. */
      id: string;
      /** A dictionary to edit and add fields to a document. */
      update: { [key: string]: unknown };
      /** Whether to include insert date as a field 'insert_date_'. */
      insert_date?: boolean;
    };
    /** Base class for all abstractmodels */
    UpdateDeployableBody: {
      /** The deployable configuration. */
      configuration?: { [key: string]: unknown };
      /** Unique name of dataset, */
      dataset_id?: string;
      /** Whether to overwrite document if it exists. */
      overwrite?: boolean;
      /** This is prioritised over overwrite. If True, adds new fields. */
      upsert?: boolean;
    };
    /** Base class for each response model */
    UpdateDeployableResponse: {
      configuration?: { [key: string]: unknown };
      status?: string;
      message?: string;
    };
    /** Base class for each response model */
    UpdateDocumentResponse: {
      status: string;
      message: string;
    };
    /** Base class for all abstractmodels */
    UpdateWhereBody: {
      /** Updates to make to the documents. It should be specified in a format of {"field_name": "value"}. e.g. {"item.status" : "Sold Out"} */
      updates: { [key: string]: unknown };
      /** Query for filtering the search results */
      filters?: { [key: string]: unknown }[];
    };
    ValidationError: {
      loc: string[];
      msg: string;
      type: string;
    };
    /** Multivector Query Refactor */
    VectorQuery: {
      /** Vector, a list/array of floats that represents a piece of data. */
      vector: number[];
      /** The vector field to search in. It can either be an array of strings (automatically equally weighted) (e.g. ['check_vector_', 'yellow_vector_']) or it is a dictionary mapping field to float where the weighting is explicitly specified (e.g. {'check_vector_': 0.2, 'yellow_vector_': 0.5}) */
      fields: Partial<string[]> & Partial<{ [key: string]: number }>;
      /** Alias used to name a vector field. Belongs in field_{alias}_vector_ */
      alias?: string;
    };
    /** Base class for each response model */
    VectorResponse: {
      /** Vector, a list/array of floats that represents a piece of data. */
      vector: number[];
    };
    /** Base class for all abstractmodels */
    VectorizeEncodeDataset: {
      /** Fields to remove ['random_field', 'another_random_field']. Defaults to no removes */
      fields?: string[];
      /** Filters to run against */
      filters?: unknown[];
      /** If True, re-runs encoding on whole dataset. */
      refresh?: boolean;
      /** Alias used to name a vector field. Belongs in field_{alias}_vector_ */
      alias?: string;
      /** batch for each encoding. Change at your own risk. */
      chunksize?: number;
      /** The chunk field. If the chunk field is specified, the field to be encoded should not include the chunk field. */
      chunk_field?: string;
      /** Model ID to use for vectorizing (encoding.) */
      model_id: string;
    };
    /** Base class for all abstractmodels */
    WordCloudModel: {
      /** Unique name of dataset */
      dataset_id: string;
      /** The field on which to build NGrams */
      fields: unknown[];
      /** The number of words fo combine */
      n?: number;
      /** The most common number of n-gram terms */
      most_common?: number;
      /** Size of each page of results */
      page_size?: number;
      /** Fields to include in the search results, empty array/list means all fields. */
      select_fields?: unknown[];
      /** Include vectors in the search results */
      include_vector?: boolean;
      /** Query for filtering the search results */
      filters?: unknown[];
      /** Additional stopwords to add */
      additional_stopwords?: unknown[];
    };
  };
}

export interface operations {
  /** Creates a read only key for your project. Make sure to save the api key somewhere safe. When doing a search the admin username should still be used. */
  request_read_api_key_api_admin_request_read_api_key_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["RequestReadApiKeyBody"];
      };
    };
  };
  /** Copy a dataset from another user's projects into your project. This is considered a project job */
  copy_foreign_dataset_admin_copy_foreign_dataset_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["CopyForeignDatasetBody"];
      };
    };
  };
  /**
   * A dataset can store documents to be **searched, retrieved, filtered and aggregated** (similar to Collections in MongoDB, Tables in SQL, Indexes in ElasticSearch).
   *
   * A powerful and core feature of VecDB is that you can store both your metadata and vectors in the same document.
   * When specifying the schema of a dataset and inserting your own vector use the suffix (ends with) **"\_vector\_"** for the field name, and specify the length of the vector in dataset_schema.
   *
   * For example:
   *
   *     {
   *         "product_image_vector_": 1024,
   *         "product_text_description_vector_" : 128
   *     }
   *
   * These are the field types supported in our datasets: **["text", "numeric", "date", "dict", "chunks", "vector", "chunkvector"]**.
   *
   * For example:
   *
   *     {
   *         "product_text_description" : "text",
   *         "price" : "numeric",
   *         "created_date" : "date",
   *         "product_texts_chunk_": "chunks",
   *         "product_text_chunkvector_" : 1024
   *     }
   *
   * You don't have to specify the schema of every single field when creating a dataset, as VecDB will automatically detect the appropriate data type for each field (vectors will be automatically identified by its **"\_vector\_"** suffix). Infact you also don't always have to use this endpoint to create a dataset as **\/datasets/bulk_insert** will infer and create the dataset and schema as you insert new documents.
   *
   * Note:
   *  * A dataset name/id can only contain undercase letters, dash, underscore and numbers.
   *  * "\_id" is reserved as the key and id of a document.
   *  * Once a schema is set for a dataset it cannot be altered. If it has to be altered, utlise the copy dataset endpoint.
   *
   * For more information about vectors check out the 'Vectorizing' section, **\/services/search/vector** or out blog at [https://relevance.ai/blog](https://relevance.ai/blog).
   * For more information about chunks and chunk vectors check out **\/services/search/chunk**.
   */
  create_dataset_api_datasets_create_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["CreateDatasetResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["CreateDatasetBody"];
      };
    };
  };
  /** Create a dataset and infer its schema from a single document. Refer to _/datasets_/create for more about how a dataset is created. */
  infer_schema_api_datasets_infer_schema_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["CreateDatasetResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["InferSchemaBody"];
      };
    };
  };
  /** Returns the schema of a dataset. Refer to _/datasets_/create for different field types available in a VecDB schema. */
  schema_api_datasets__dataset_id__schema_get: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Deletes a dataset. */
  delete_dataset_api_datasets_delete_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["DatasetDeleteBody"];
      };
    };
  };
  /** List all datasets in a project that you are authorized to read/write. */
  list_datasets_api_datasets_list_get: {
    parameters: {
      query: {
        /** Sort by created at date. By default shows the newest datasets. Set asc=False to get oldest dataset. */
        sort_by_created_at_date?: boolean;
        /** Whether to sort results by ascending or descending order */
        asc?: boolean;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /**
   * Returns a page of datasets and in detail the dataset's associated information that you are authorized to read/write. The information includes:
   *
   * | Information      | Description |
   * | ----------- | ----------- |
   * | schema      | Data schema of a dataset. (returns same data as **\/dataset/{dataset_id}/schema**) |
   * | metadata   | Metadata of a dataset. (returns same data as **\/dataset/{dataset_id}/metadata**) |
   * | stats   | Statistics of number of documents and size of a dataset. (returns same data as **\/dataset/{dataset_id}/monitor/stats**) |
   * | vector_health   | Number of zero vectors stored. (returns same data as **\/dataset/{dataset_id}/monitor/health**) |
   * | schema_stats   | Fields and number of documents missing/not missing for that field. (returns same data as **\/dataset/{dataset_id}/monitor/stats**) |
   * | active_jobs   | All active jobs/tasks on the dataset. (returns same data as **\/dataset/{dataset_id}/tasks/list**) |
   */
  list_collections_api_datasets_list_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["ListDatasetsBody"];
      };
    };
  };
  /** Search datasets by their names with a traditional keyword search. */
  search_datasets_api_datasets_search_get: {
    parameters: {
      query: {
        /** Any string that belongs to part of a dataset. */
        query: string;
        /** Sort by created at date. By default shows the newest datasets. Set asc=False to get oldest dataset. */
        sort_by_created_at_date?: boolean;
        /** Whether to sort results by ascending or descending order */
        asc?: boolean;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Takes a high level aggregation of every field, return their unique values and frequencies. This is used to help create the filter bar for search. */
  facets_api_datasets__dataset_id__facets_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["FacetsBody"];
      };
    };
  };
  /** Clone a dataset into a new dataset. You can use this to rename fields and change data schemas. This is considered a project job. */
  clone_dataset_api_datasets__dataset_id__clone_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["CopyCollectionBody"];
      };
    };
  };
  /**  */
  store_encoders_pipeline_api_datasets__dataset_id__store_encoders_pipeline_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["StoreEncodersPipelineBody"];
      };
    };
  };
  /** Retrieve the mapping of vectors generated through fields, dictionary, array, etc */
  dataset_vector_mappings_api_datasets__dataset_id__vector_mappings_get: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Retrieve the mapping of vectors generated through fields, dictionary, array, etc. */
  dataset_vector_mappings_api_post_datasets__dataset_id__vector_mappings_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["PostVectorMappings"];
      };
    };
  };
  /**
   * * When inserting the document you can optionally specify your own id for a document by using the field name **"\_id"**, if not specified a random id is assigned.
   *  * When inserting or specifying vectors in a document use the suffix (ends with)  **"\_vector\_"** for the field name. e.g. "product\_description\_vector\_".
   *  * When inserting or specifying chunks in a document the suffix (ends with)  **"\_chunk\_"** for the field name. e.g. "products\_chunk\_".
   *  * When inserting or specifying chunk vectors in a document's chunks use the suffix (ends with)  **"\_chunkvector\_"** for the field name. e.g. "products_chunk_.product\_description\_chunkvector\_".
   * For multiple document insert version of this request use **\/datasets/{dataset_id}/documents/bulk_insert**.
   */
  insert_api_datasets__dataset_id__documents_insert_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["InsertDocumentResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["InsertBody"];
      };
    };
  };
  /**
   * * When inserting the document you can optionally specify your own id for a document by using the field name **"\_id"**, if not specified a random id is assigned.
   *  * When inserting or specifying vectors in a document use the suffix (ends with)  **"\_vector\_"** for the field name. e.g. "product\_description\_vector\_".
   *  * When inserting or specifying chunks in a document the suffix (ends with)  **"\_chunk\_"** for the field name. e.g. "products\_chunk\_".
   *  * When inserting or specifying chunk vectors in a document's chunks use the suffix (ends with)  **"\_chunkvector\_"** for the field name. e.g. "products_chunk_.product\_description\_chunkvector\_".
   *
   * Try to keep each batch of documents to insert under 200mb to avoid the insert timing out. For single document insert version of this request use **\/datasets/{dataset_id}/documents/insert**.
   */
  bulk_insert_api_datasets__dataset_id__documents_bulk_insert_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["BulkInsertResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["BulkInsertBody"];
      };
    };
  };
  /** Retrieve a document by its ID ("_id" field). This will retrieve the document faster than a filter applied on the "_id" field. For multiple id lookup version of this request use **\/datasets/{dataset_id}/documents/bulk_get**. */
  id_lookup_api_datasets__dataset_id__documents_get_get: {
    parameters: {
      path: {
        dataset_id: string;
      };
      query: {
        /** ID of a document in a dataset. */
        id: string;
        /** Include vectors in the search results */
        include_vector?: boolean;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["DocumentResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Retrieve documents by their IDs ("_id" field). This will retrieve the documents faster than a filter applied on the "_id" field. For single id lookup version of this request use **\/datasets/{dataset_id}/documents/get**. */
  bulk_id_lookup_api_datasets__dataset_id__documents_bulk_get_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["DocumentsResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["BulkGetDocuments"];
      };
    };
  };
  /** Retrieve documents from a specified dataset. Cursor is provided to retrieve even more documents. Loop through it to retrieve all documents in the dataset. For pagination support refer to **\/datasets/{dataset_id}/documents/paginate**. */
  retrieve_documents_api_datasets__dataset_id__documents_list_get: {
    parameters: {
      path: {
        dataset_id: string;
      };
      query: {
        /** Fields to include in the search results, empty array/list means all fields. */
        select_fields?: unknown[];
        /** Cursor to paginate the document retrieval */
        cursor?: string;
        /** Size of each page of results */
        page_size?: number;
        /** Include vectors in the search results */
        include_vector?: boolean;
        /** Random Seed for retrieving random documents. */
        random_state?: number;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /**
   * Retrieve documents with filters.
   * Cursor is provided to retrieve even more documents. Loop through it to retrieve all documents in the database.
   * Filter is used to retrieve documents that match the conditions set in a filter query. This is used in advance search to filter the documents that are searched.
   *
   * The filters query is a json body that follows the schema of:
   *
   *     [
   *         {'field' : <field to filter>, 'filter_type' : <type of filter>, "condition":"==", "condition_value":"america"},
   *         {'field' : <field to filter>, 'filter_type' : <type of filter>, "condition":">=", "condition_value":90},
   *     ]
   *
   * These are the available filter_type types: **["contains", "category", "categories", "exists", "date", "numeric", "ids"]**
   *
   * 1. **"contains"**: for filtering documents that contains a string.
   *         {'field' : 'item_brand', 'filter_type' : 'contains', "condition":"==", "condition_value": "samsu"}
   * 2. **"exact_match"/"category"**: for filtering documents that matches a string or list of strings exactly.
   *         {'field' : 'item_brand', 'filter_type' : 'category', "condition":"==", "condition_value": "sumsung"}
   * 3. **"categories"**: for filtering documents that contains any of a category from a list of categories.
   *         {'field' : 'item_category_tags', 'filter_type' : 'categories', "condition":"==", "condition_value": ["tv", "smart", "bluetooth_compatible"]}
   * 4. **"exists"**: for filtering documents that contains a field.
   *         {'field' : 'purchased', 'filter_type' : 'exists', "condition":"==", "condition_value":" "}
   * If you are looking to filter for documents where a field doesn't exist, run this:
   *         {'field' : 'purchased', 'filter_type' : 'exists', "condition":"!=", "condition_value":" "}
   * 5. **"date"**: for filtering date by date range.
   *         {'field' : 'insert_date_', 'filter_type' : 'date', "condition":">=", "condition_value":"2020-01-01"}
   * 6. **"numeric"**: for filtering by numeric range.
   *         {'field' : 'price', 'filter_type' : 'numeric', "condition":">=", "condition_value":90}
   * 7. **"ids"**: for filtering by document ids.
   *         {'field' : 'ids', 'filter_type' : 'ids', "condition":"==", "condition_value":["1", "10"]}
   * 8. **"or"**: for filtering with multiple conditions
   *         {'filter_type' : 'or',
   * "condition_value": [{'field' : 'price', 'filter_type' : 'numeric', "condition":"<=", "condition_value":90},
   * {'field' : 'price', 'filter_type' : 'numeric', "condition":">=", "condition_value":150}]}
   *
   * These are the available conditions:
   *
   *     "==", "!=", ">=", ">", "<", "<="
   *
   * If you are looking to combine your filters with multiple ORs, simply add the following inside the query
   * `{"strict":"must_or"}`.
   *
   * For pagination support refer to **\/datasets/{dataset_id}/documents/paginate**.
   */
  retrieve_documents_with_filters_api_datasets__dataset_id__documents_get_where_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["GetWhereDocuments"];
      };
    };
  };
  /** Retrieve documents with filters and support for pagination. For more information about filters refer to **\/datasets/{dataset_id}/documents/get_where**. */
  retrieve_documents_with_filters_api_datasets__dataset_id__documents_paginate_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["PaginateDocuments"];
      };
    };
  };
  /** Look up in bulk if the ids exists in the dataset, returns all the missing one as a list. */
  bulk_missing_id_api_datasets__dataset_id__documents_get_missing_get: {
    parameters: {
      path: {
        dataset_id: string;
      };
      query: {
        /** IDs of documents */
        ids: string[];
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": string[];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Edit by providing a key value pair of fields you are adding or changing. For update multiple documents refer to **\/datasets/{dataset_id}/documents/bulk_update**. */
  update_document_api_datasets__dataset_id__documents_update_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["UpdateDocumentResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["UpdateBody"];
      };
    };
  };
  /** Edits documents by providing a key value pair of fields you are adding or changing, make sure to include the "_id" in the documents. For update a single document refer to **\/datasets/{dataset_id}/documents/update** or updating documents by filters refer to **\/datasets/{dataset_id}/documents/update_where**. */
  bulk_update_documents_api_datasets__dataset_id__documents_bulk_update_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["BulkUpdateResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["BulkUpdateBody"];
      };
    };
  };
  /** Delete a document by its id. For deleting multiple documents refer to **\/datasets/{dataset_id}/documents/bulk_delete**. */
  delete_api_datasets__dataset_id__documents_delete_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["DeleteDocumentBody"];
      };
    };
  };
  /** Delete a list of documents by their IDs. For deleting a single document refer to **\/datasets/{dataset_id}/documents/delete** or deleting documents by filters refer to **\/datasets/{dataset_id}/documents/delete_where**. */
  bulk_delete_api_datasets__dataset_id__documents_bulk_delete_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["BulkDeleteResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["BulkDeleteDocuments"];
      };
    };
  };
  /** Delete fields in a document in a dataset by its id. */
  delete_fields_api_datasets__dataset_id__documents_delete_fields_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["DeleteFieldsBody"];
      };
    };
  };
  /** Updates documents by filters. The updates to make to the documents that is returned by a filter. The updates should be specified in a format of {"field_name": "value"}. e.g. {"item.status" : "Sold Out"}. For more information about filters refer to **\/datasets/{dataset_id}/documents/get_where**. */
  update_by_filters_api_datasets__dataset_id__documents_update_where_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["UpdateWhereBody"];
      };
    };
  };
  /** Delete documents by filters. For more information about filters refer to **\/datasets/{dataset_id}/documents/get_where**. */
  delete_by_filters_api_datasets__dataset_id__documents_delete_where_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["DeleteWhereBody"];
      };
    };
  };
  /** Retreives metadata about a dataset. Notably description, data source, etc */
  collection_metadata_api_datasets__dataset_id__metadata_get: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Edit and add metadata about a dataset. Notably description, data source, etc */
  post_collection_metadata_api_datasets__dataset_id__metadata_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["MetadataPostBody"];
      };
    };
  };
  dataset_schema_stats_api_datasets__dataset_id__monitor_stats_get: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Gives you a summary of the health of your vectors, e.g. how many documents with vectors are missing, how many documents with zero vectors */
  dataset_vector_health_api_datasets__dataset_id__monitor_health_get: {
    parameters: {
      path: {
        dataset_id: string;
      };
      query: {
        /** Whether to include stats about zero vectors. If False, only includes information on missing vectors. */
        include_zero_vectors?: boolean;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /**
   * Aggregate the logs for a dataset
   *
   * The response returned has the following fields.
   *
   *     [{'frequency': 958, 'insert_date': 1630159200000},...]
   */
  aggregate_logs_api_datasets__dataset_id__monitor_usage_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["AggregateLogQuery"];
      };
    };
  };
  /** Tasks unlock the power of VecDb AI by adding a lot more new functionality with a flexible way of searching. */
  tasks_api_datasets__dataset_id__tasks_create_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": Partial<components["schemas"]["TaskResponse"]> &
            Partial<{ [key: string]: unknown }>;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": Partial<{ [key: string]: unknown }> &
          Partial<components["schemas"]["Clusterer"]> &
          Partial<components["schemas"]["CategoriesEncoder"]> &
          Partial<components["schemas"]["DictionaryEncoder"]> &
          Partial<components["schemas"]["NumericFieldsEncoder"]> &
          Partial<components["schemas"]["TextEncoder"]> &
          Partial<components["schemas"]["TextImageEncoder"]> &
          Partial<components["schemas"]["TextMultiEncoder"]> &
          Partial<components["schemas"]["ImageTextEncoder"]> &
          Partial<components["schemas"]["TextSentiment"]> &
          Partial<components["schemas"]["BulkPDFProcessing"]> &
          Partial<components["schemas"]["BulkDocProcessing"]>;
      };
    };
  };
  /** List and get a history of all the jobs and its job_id, parameters, start time, etc. */
  list_collection_jobs_api_datasets__dataset_id__tasks_list_get: {
    parameters: {
      path: {
        dataset_id: string;
      };
      query: {
        show_active_only?: boolean;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Get status of a collection level job. Whether its starting, running, failed or finished. */
  tasks_status_api_datasets__dataset_id__tasks__task_id__status_get: {
    parameters: {
      path: {
        dataset_id: string;
        task_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": Partial<components["schemas"]["TaskResponse"]> &
            Partial<{ [key: string]: unknown }>;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Queue the encoding of a dataset using the method given by model_id. */
  encode_by_model_api_datasets__dataset_id__vectorize_post: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["VectorizeEncodeDataset"];
      };
    };
  };
  /**
   * Check the status of an existing encoding task on the given dataset.
   *
   * The required task_id was returned in the original encoding request
   * such as /vectorize.
   */
  task_status_by_model_api_datasets__dataset_id__task_status_get: {
    parameters: {
      path: {
        dataset_id: string;
      };
      query: {
        /** The task ID of the earlier queued vectorize task */
        task_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /**
   * List the tasks being encoded for the dataset_id that
   * you are authorized to read.
   *
   * If dataset_id is the wildcard "*" then all tasks for the
   * user project are listed.
   */
  list_tasks_api_datasets__dataset_id__tasks_get: {
    parameters: {
      path: {
        dataset_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /**
   * Allows you to leverage vector similarity search to create a semantic search engine.
   * Powerful features of VecDB vector search:
   *  1. Multivector search that allows you to search with multiple vectors and give each vector a different weight. e.g. Search with a product image vector and text description vector to find the most similar products by what it looks like and what its described to do. You can also give weightings of each vector field towards the search, e.g. image\_vector\_ weights 100%, whilst description\_vector\_ 50%.
   *
   *     An example of a simple multivector query:
   *
   *         [
   *             {"vector": [0.12, 0.23, 0.34], "fields": ["name_vector_"], "alias":"text"},
   *             {"vector": [0.45, 0.56, 0.67], "fields": ["image_vector_"], "alias":"image"},
   *         ]
   *
   *     An example of a weighted multivector query:
   *
   *         [
   *             {"vector": [0.12, 0.23, 0.34], "fields": {"name_vector_":0.6}, "alias":"text"},
   *             {"vector": [0.45, 0.56, 0.67], "fields": {"image_vector_"0.4}, "alias":"image"},
   *         ]
   *
   *     An example of a weighted multivector query with multiple fields for each vector:
   *
   *         [
   *             {"vector": [0.12, 0.23, 0.34], "fields": {"name_vector_":0.6, "description_vector_":0.3}, "alias":"text"},
   *             {"vector": [0.45, 0.56, 0.67], "fields": {"image_vector_"0.4}, "alias":"image"},
   *         ]
   *
   *  2. Utilise faceted search with vector search. For information on how to apply facets/filters checkout **\/datasets/{dataset_id}/documents/get_where**.
   *
   *  3. Sum Fields option to adjust whether you want multiple vectors to be combined in the scoring or compared in the scoring. e.g. image\_vector\_ + text\_vector\_ or image\_vector\_ vs text\_vector\_.
   *
   *     setting `sum_fields=True`:
   *      * Multi-vector search allows you to obtain search scores by taking the sum of these scores.
   *      * TextSearchScore + ImageSearchScore = SearchScore
   *      * We then rank by the new SearchScore, so for searching 1000 documents there will be 1000 search scores and results
   *
   *     setting `sum_fields=False`:
   *      * Multi vector search but not summing the score, instead including it in the comparison!
   *      * TextSearchScore = SearchScore1
   *      * ImagSearcheScore = SearchScore2
   *      * We then rank by the 2 new SearchScore, so for searching 1000 documents there should be 2000 search scores and results.
   *
   *  4. Personalization with positive and negative document ids.
   *
   *     For more information about the positive and negative document ids to personalize checkout the **\/services/recommend/vector** endpoint.
   *
   * For more even more advanced configuration and customisation of vector search, reach out to us at dev@relevance.ai and learn about our new advanced_vector_search.
   */
  vector_search_api_services_search_vector_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["AdvancedSearchQueryBody"];
      };
    };
  };
  /**
   * Traditional Faceted Keyword Search with edit distance/fuzzy matching.
   *
   * For information on how to apply facets/filters checkout **\/datasets/{dataset_id}/documents/get_where**.
   *
   * For information on how to construct the facets section for your search bar checkout out **\/datasets/{dataset_id}/facets**.
   */
  traditional_search_api_services_search_traditional_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["SearchResults"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["TraditionalSearchBody"];
      };
    };
  };
  /**
   * Combine the best of both traditional keyword faceted search with semantic vector search to create the best search possible.
   *
   * For information on how to use vector search **\/services/search/vector**.
   *
   * For information on how to use traditional keyword faceted search **\/services/search/traditional**.
   */
  hybrid_search_api_services_search_hybrid_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["SearchResults"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["AdvancedHybridSearchBody"];
      };
    };
  };
  /**
   * A more automated hybrid search with a few extra things that automatically adjusts some of the key parameters for more automated and good out of the box results.
   *
   * For information on how to configure semantic search checkout **\/services/search/hybrid**.
   */
  hybrid_search_api_services_search_semantic_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["SearchResults"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["SemanticSearchBody"];
      };
    };
  };
  /**
   * Chunks are data that has been divided into different units. e.g. A paragraph is made of many sentence chunks, a sentence is made of many word chunks, an image frame in a video. By searching through chunks you can pinpoint more specifically where a match is occuring.
   * When creating a chunk in your document use the suffix "_chunk_" and "_chunkvector_". An example of a document with chunks:
   *
   *     {
   *         "_id" : "123",
   *         "title" : "Lorem Ipsum Article",
   *         "description" : "Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.",
   *         "description_vector_" : [1.1, 1.2, 1.3],
   *         "description_sentence_chunk_" : [
   *             {"sentence_id" : 0, "sentence_chunkvector_" : [0.1, 0.2, 0.3], "sentence" : "Lorem Ipsum is simply dummy text of the printing and typesetting industry."},
   *             {"sentence_id" : 1, "sentence_chunkvector_" : [0.4, 0.5, 0.6], "sentence" : "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book."},
   *             {"sentence_id" : 2, "sentence_chunkvector_" : [0.7, 0.8, 0.9], "sentence" : "It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged."},
   *         ]
   *     }
   *
   * For combining chunk search with other search checkout **\/services/search/advanced_chunk**.
   */
  chunk_search_api_services_search_chunk_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["SearchResults"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["ChunkSearchQuery"];
      };
    };
  };
  /**
   * Multistep chunk search involves a vector search followed by chunk search, used to accelerate chunk searches or to identify context before delving into relevant chunks. e.g. Search against the paragraph vector first then sentence chunkvector after.
   *
   * For more information about chunk search checkout **\/services/search/chunk**.
   *
   * For more information about vector search checkout **\/services/search/vector**.
   */
  advanced_multistep_chunk_search_api_services_search_multistep_chunk_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["ChunkSearchResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["AdvancedMultiStepChunkSearchQueryBody"];
      };
    };
  };
  /**
   * A more advanced chunk search to be able to combine vector search and chunk search in many different ways.
   *
   *     chunk_query = {
   *         "chunk" : "some.test",
   *         "queries" : [
   *             {"vector" : vec1, "fields": {"some.test.some_chunkvector_":1},
   *             "traditional_query" : {"text":"python", "fields" : ["some.test.test_words"], "traditional_weight": 0.3},
   *             "metric" : "cosine"},
   *             {"vector" : vec, "fields": ["some.test.tt.some_other_chunkvector_"],
   *             "traditional_query" : {"text":"jumble", "fields" : ["some.test.test_words"], "traditional_weight": 0.3},
   *             "metric" : "cosine"},
   *         ]
   *     }
   * Example 2 (combines normal vector search with chunk search):
   *
   *     chunk_query = {
   *         "queries" : [
   *             {
   *                 "queries": [
   *                     {
   *                         "vector": vec1,
   *                         "fields": {
   *                             "some.test.some_chunkvector_": 0.9
   *                         },
   *                         "traditional_query": {
   *                             "text": "python",
   *                             "fields": [
   *                                 "some.test.test_words"
   *                             ],
   *                             "traditional_weight": 0.3
   *                         },
   *                         "metric": "cosine"
   *                     }
   *                 ],
   *                 "chunk": "some.test",
   *             },
   *             {
   *                 "vector" : vec,
   *                 "fields": {
   *                     ".some_vector_" : 0.1},
   *                     "metric" : "cosine"
   *                     },
   *             ]
   *         }
   */
  advanced_chunk_search_api_services_search_advanced_chunk_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["ChunkSearchResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["AdvancedChunkSearch"];
      };
    };
  };
  /**
   * Performs a vector hybrid search and then an advanced chunk search.
   *
   * Chunk Search allows one to search through chunks inside a document. The major difference between chunk search and normal search in Vector AI is that it relies on the _chunkvector_ field.
   * Chunk Vector Search. Search with a multiple chunkvectors for the most similar documents.
   * Chunk search also supports filtering to only search through filtered results and facets to get the overview of products available when a minimum score is set.
   *
   * Example 1 (Hybrid chunk search):
   *
   *     chunk_query1= {
   *         "chunk" : "some.test",
   *         "queries" : [
   *             {"vector" : vec1, "fields": {"some.test.some_chunkvector_":1},
   *             "traditional_query" : {"text":"python", "fields" : ["some.test.test_words"], "traditional_weight": 0.3},
   *             "metric" : "cosine"},
   *             {"vector" : vec, "fields": ["some.test.tt.some_other_chunkvector_"],
   *             "traditional_query" : {"text":"jumble", "fields" : ["some.test.test_words"], "traditional_weight": 0.3},
   *             "metric" : "cosine"},
   *         ]
   *     }
   *
   * Example 2 (combines normal vector search with chunk search):
   *
   *     chunk_query1= {
   *     "queries" : [
   *         {
   *             "queries": [
   *                 {
   *                     "vector": vec1,
   *                     "fields": {
   *                         "some.test.some_chunkvector_": 0.9
   *                     },
   *                     "traditional_query": {
   *                         "text": "python",
   *                         "fields": [
   *                             "some.test.test_words"
   *                         ],
   *                         "traditional_weight": 0.3
   *                     },
   *                     "metric": "cosine"
   *                 }
   *             ],
   *             "chunk": "some.test",
   *         },
   *         {
   *             "vector" : vec,
   *             "fields": {
   *                 ".some_vector_" : 0.1},
   *                 "metric" : "cosine"
   *                 },
   *         ]
   *     }
   */
  advanced_multistep_chunk_search_api_services_search_advanced_multistep_chunk_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["ChunkSearchResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["AdvancedMultiStepChunkSearch"];
      };
    };
  };
  /**
   * This will first perform an advanced search and then cluster the top X (page_size) search results.
   * Results are returned as such:
   * Once you have the clusters:
   *
   * ```
   * Cluster 0: [A, B, C]
   * Cluster 1: [D, E]
   * Cluster 2: [F, G]
   * Cluster 3: [H, I]
   * ```
   * (Note, each cluster is ordered by highest to lowest search score.
   *
   * This intermediately returns:
   *
   * ```
   * results_batch_1: [A, H, F, D] (ordered by highest search score)
   * results_batch_2: [G, E, B, I] (ordered by highest search score)
   * results_batch_3: [C]
   * ```
   *
   * This then returns the final results:
   *
   * ```
   * results: [A, H, F, D, G, E, B, I, C]
   */
  vector_search_api_services_search_diversity_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["SearchResults"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["DiversityModel"];
      };
    };
  };
  /**
   * Vector Search based recommendations are done by extracting the vectors of the documents ids specified performing some vector operations and then searching the dataset with the resultant vector.
   *
   * This allows us to not only do recommendations but personalized and weighted recommendations here are a couple of different scenarios and what the queries would look like for those:
   *
   * 1. Recommendations Personalized by single liked product:
   *
   *     `positive_document_ids=['A']`
   *
   *     -> Document ID A Vector = Search Query
   *
   * 2. Recommendations Personalized by multiple liked product:
   *
   *     `positive_document_ids=['A', 'B']`
   *
   *     -> Document ID A Vector + Document ID B Vector = Search Query
   *
   * 3. Recommendations Personalized by multiple liked product and disliked products:
   *
   *     `positive_document_ids=['A', 'B'], negative_document_ids=['C', 'D']`
   *
   *     -> (Document ID A Vector + Document ID B Vector) - (Document ID C Vector + Document ID C Vector) = Search Query
   *
   * 4. Recommendations Personalized by multiple liked product and disliked products with weights:
   *
   *     `positive_document_ids={'A':0.5, 'B':1}, negative_document_ids={'C':0.6, 'D':0.4}`
   *
   *     -> (Document ID A Vector * 0.5 + Document ID B Vector * 1) - (Document ID C Vector * 0.6 + Document ID D Vector * 0.4) = Search Query
   *
   * You can change the operator between vectors with vector_operation:
   *
   * e.g. `positive_document_ids=['A', 'B'], negative_document_ids=['C', 'D'], vector_operation='multiply'`
   *
   * -> (Document ID A Vector * Document ID B Vector) - (Document ID C Vector * Document ID D Vector) = Search Query
   */
  vector_recommend_api_services_recommend_vector_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["RecommendResults"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["AdvancedRecommendBody"];
      };
    };
  };
  /**
   * Vector Search based recommendations are done by extracting the vectors of the documents ids specified performing some vector operations and then searching the dataset with the resultant vector.
   *
   * This allows us to not only do recommendations but personalized and weighted recommendations.
   *
   * Diversity recommendation increases the variety within the recommendations via clustering. Search results are clustered and the top k items in each cluster are selected.
   * The main clustering parameters are
   * `cluster_vector_field` and `n_clusters`, the vector field on which to perform clustering and number of clusters respectively.
   *
   * Here are a couple of different scenarios and what the queries would look like for those:
   *
   * 1. Recommendations Personalized by single liked product:
   *
   *     `positive_document_ids=['A']`
   *
   *     -> Document ID A Vector = Search Query
   *
   * 2. Recommendations Personalized by multiple liked product:
   *
   *     `positive_document_ids=['A', 'B']`
   *
   *     -> Document ID A Vector + Document ID B Vector = Search Query
   *
   * 3. Recommendations Personalized by multiple liked product and disliked products:
   *
   *     `positive_document_ids=['A', 'B'], negative_document_ids=['C', 'D']`
   *
   *     -> (Document ID A Vector + Document ID B Vector) - (Document ID C Vector + Document ID C Vector) = Search Query
   *
   * 4. Recommendations Personalized by multiple liked product and disliked products with weights:
   *
   *     `positive_document_ids={'A':0.5, 'B':1}, negative_document_ids={'C':0.6, 'D':0.4}`
   *
   *     -> (Document ID A Vector * 0.5 + Document ID B Vector * 1) - (Document ID C Vector * 0.6 + Document ID D Vector * 0.4) = Search Query
   *
   * You can change the operator between vectors with vector_operation:
   *
   * e.g. `positive_document_ids=['A', 'B'], negative_document_ids=['C', 'D'], vector_operation='multiply'`
   *
   * -> (Document ID A Vector * Document ID B Vector) - (Document ID C Vector * Document ID D Vector) = Search Query
   */
  vector_diversity_recommend_api_services_recommend_diversity_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["AdvancedDiversityRecommendBody"];
      };
    };
  };
  /**
   * Aggregation/Groupby of a collection using an aggregation query.
   * The aggregation query is a json body that follows the schema of:
   *
   *     {
   *         "groupby" : [
   *             {"name": <alias>, "field": <field in the collection>, "agg": "category"},
   *             {"name": <alias>, "field": <another groupby field in the collection>, "agg": "numeric"}
   *         ],
   *         "metrics" : [
   *             {"name": <alias>, "field": <numeric field in the collection>, "agg": "avg"}
   *             {"name": <alias>, "field": <another numeric field in the collection>, "agg": "max"}
   *         ]
   *     }
   *     For example, one can use the following aggregations to group score based on region and player name.
   *     {
   *         "groupby" : [
   *             {"name": "region", "field": "player_region", "agg": "category"},
   *             {"name": "player_name", "field": "name", "agg": "category"}
   *         ],
   *         "metrics" : [
   *             {"name": "average_score", "field": "final_score", "agg": "avg"},
   *             {"name": "max_score", "field": "final_score", "agg": "max"},
   *             {'name':'total_score','field':"final_score", 'agg':'sum'},
   *             {'name':'average_deaths','field':"final_deaths", 'agg':'avg'},
   *             {'name':'highest_deaths','field':"final_deaths", 'agg':'max'},
   *         ]
   *     }
   * - "groupby" is the fields you want to split the data into. These are the available groupby types:
   *     - category" : groupby a field that is a category
   *     - numeric: groupby a field that is a numeric
   *     - wordcloud: groupby the words. You can also additionally include stop words in your aggregation
   *     if you add `remove_words` as a field.
   *
   *     {
   *         "name": "wordcloud_research",
   *         "field": "title",
   *         "agg": "wordcloud",
   *         "remove_words": ["learning"]
   *     }
   *
   *
   * - "metrics" is the fields you want to metrics you want to calculate in each of those, every aggregation includes a frequency metric. These are the available metric types:
   *     - "avg", "max", "min", "sum", "cardinality"
   *
   * The response returned has the following in descending order.
   *
   * IF you want to return documents, specify a "group_size" parameter and a "select_fields" parameter if you want to limit the specific fields chosen.
   * This looks as such:
   *
   *     {
   *       'groupby':[
   *         {'name':'Manufacturer','field':'manufacturer','agg':'category',
   *         'group_size': 10, 'select_fields': ["name"]},
   *       ],
   *       'metrics':[
   *         {'name':'Price Average','field':'price','agg':'avg'},
   *       ],
   *     }
   *
   *     {"title": {"title": "books", "frequency": 200, "documents": [{...}, {...}]}, {"title": "books", "frequency": 100, "documents": [{...}, {...}]}}
   */
  aggregate_v2_api_services_aggregate_aggregate_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["AggregateQueryV2Commons"];
      };
    };
  };
  /** Retrieves a list of cluster centroids */
  cluster_centroids_api_services_cluster_centroids_list_get: {
    parameters: {
      query: {
        /** Unique name of dataset */
        dataset_id: string;
        /** The vector field where a clustering task was run. */
        vector_field: string;
        /** Alias is used to name a cluster */
        alias?: string;
        /** Size of each page of results */
        page_size?: number;
        /** Cursor to paginate the document retrieval */
        cursor?: string;
        /** Include vectors in the search results */
        include_vector?: boolean;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Retrieves a list of cluster centroids */
  cluster_centroids_api_v2_services_cluster_centroids_list_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["ClustercentroidsList"];
      };
    };
  };
  /** Retrieve the cluster centroids by IDs */
  cluster_centroids_get_api_services_cluster_centroids_get_get: {
    parameters: {
      query: {
        /** Unique name of dataset */
        dataset_id: string;
        /** List of cluster IDs */
        cluster_ids: string[];
        /** The vector field where a clustering task was run. */
        vector_field: string;
        /** Alias is used to name a cluster */
        alias?: string;
        /** Size of each page of results */
        page_size?: number;
        /** Cursor to paginate the document retrieval */
        cursor?: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Retrieve the cluster centroids by IDs */
  cluster_centroids_get_api_services_cluster_centroids_get_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["CentroidsGetV2"];
      };
    };
  };
  /** Insert your own cluster centroids for it to be used in approximate search settings and cluster aggregations. */
  insert_cluster_centroids_2_api_services_cluster_centroids_insert_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["CentroidInsertBodyV2"];
      };
    };
  };
  /** Update a centroid by ID */
  update_centroids_api_v2_services_cluster_centroids_update_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["CentroidUpdateBodyV2"];
      };
    };
  };
  /** Delete a centroid by ID */
  delete_centroids_api_services_cluster_centroids__centroid_id__delete_get: {
    parameters: {
      path: {
        centroid_id: string;
      };
      query: {
        dataset_id: string;
        vector_field: string;
        alias: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Delete a centroid by ID */
  delete_centroids_api_services_cluster_centroids__centroid_id__delete_post: {
    parameters: {
      path: {
        centroid_id: string;
      };
      query: {
        dataset_id: string;
        vector_field: string;
        alias: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Delete centroids by dataset ID, vector field and alias */
  cluster_centroids_delete_api_services_cluster_centroids_delete_post: {
    parameters: {
      query: {
        dataset_id: string;
        vector_field: string;
        alias: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Retrieve the cluster centroids by IDs */
  cluster_centroids_get_api_services_cluster_centroids_documents_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["CentroidDocuments"];
      };
    };
  };
  /** Retrieves metadata about a dataset. notably description, data source, etc */
  centroids_metadata_get_api_services_cluster_centroids_metadata_get: {
    parameters: {
      query: {
        /** Unique name of dataset */
        dataset_id: string;
        /** It can either be an array of strings (automatically equally weighted) (e.g. ['check_vector_', 'yellow_vector_']). */
        vector_field: string;
        /** Alias used to name a vector field. Belongs in field_{alias}_vector_ */
        alias?: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** You can store the metadata about your cluster here. */
  centroids_metadata_post_api_v2_services_cluster_centroids_metadata_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["CentroidMetadataV2"];
      };
    };
  };
  /** List of documents closest from the centre. */
  centroids_list_closest_to_center_v2_services_cluster_centroids_list_closest_to_center_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["CentroidsClosestToCenterV2"];
      };
    };
  };
  /** List of documents from from the centre */
  centroids_list_furthest_from_center_v2_services_cluster_centroids_list_furthest_from_center_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["CentroidsFurthestFromCenterV2"];
      };
    };
  };
  /**
   * Takes an aggregation query and gets the aggregate of each cluster in a collection. This helps you interpret each cluster and what is in them.
   *
   * Only can be used after a vector field has been clustered with /cluster.
   */
  cluster_aggregate_api_v2_services_cluster_aggregate_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["ClusterAggregateQueryCommonsV2"];
      };
    };
  };
  /**
   * Takes a high level aggregation of every field and every cluster in a collection. This helps you interpret each cluster and what is in them.
   *
   * Only can be used after a vector field has been clustered with /cluster.
   */
  advanced_cluster_facets_api_services_cluster_facets_get: {
    parameters: {
      query: {
        /** Unique name of dataset */
        dataset_id: string;
        /** It can either be an array of strings (automatically equally weighted) (e.g. ['check_vector_', 'yellow_vector_']). */
        vector_field: string;
        /** Fields to include in the facets, if [] then all */
        facets_fields?: string[];
        /** Size of facet page */
        page_size?: number;
        /** Page of the results */
        page?: number;
        /** Whether to sort results by ascending or descending order */
        asc?: boolean;
        /** Interval for date facets */
        date_interval?: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Get a list of cluster IDs based on the relevant information */
  cluster_list_services_cluster_list_get: {
    parameters: {
      query: {
        /** Unique name of dataset */
        dataset_id: string;
        /** It can either be an array of strings (automatically equally weighted) (e.g. ['check_vector_', 'yellow_vector_']). */
        vector_field: string;
        /** Alias is used to name a cluster */
        alias?: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Get a list of cluster IDs */
  cluster_list_multi_services_cluster_list_post: {
    parameters: {
      query: {
        /** Unique name of dataset */
        dataset_id: string;
        /** The vector field to search in. It can either be an array of strings (automatically equally weighted) (e.g. ['check_vector_', 'yellow_vector_']) or it is a dictionary mapping field to float where the weighting is explicitly specified (e.g. {'check_vector_': 0.2, 'yellow_vector_': 0.5}) */
        vector_fields: string;
        /** Alias is used to name a cluster */
        alias?: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Tag documents or vectors. */
  tag_api_services_tagger_tag_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["TagBody"];
      };
    };
  };
  /** Tagging and then clustering the tags and returning one from each cluster (starting from the closest tag) */
  cluster_and_tag_api_services_tagger_diversity_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["SearchResults"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["DiversityTag"];
      };
    };
  };
  /** Recommend something via the API. */
  vector_recommend_api_services_document_diff_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["DocumentDiffResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["DocumentDiffBody"];
      };
    };
  };
  /** Predict using KNN regression. */
  predict_knn_regression_api_services_prediction_regression_knn_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["PredictKNNregressionModel"];
      };
    };
  };
  /** Predict using KNN regression from search results */
  predict_knn_regression_from_search_results_api_services_prediction_regression_knn_from_results_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["PredictKNNClassificationFromResultsModel"];
      };
    };
  };
  /**
   * For example: we choose the fields ["height", "age", "weight"]
   *     document field: {"height":180, "age":40, "weight":70, "purchases":20, "visits": 12}
   *
   *     -> <Encode the fields to vectors> ->
   *
   * | height | age | weight |
   * |--------|-----|--------|
   * | 180    | 40  | 70     |
   *
   *     document vector: {"person_characteristics_vector_": [180, 40, 70]}
   */
  encode_numeric_fields_api_services_encoders_numeric_fields_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["NumericFieldsModel"];
      };
    };
  };
  /**
   * For example: an array that represents a **movie's categories, field "movie_categories"**:
   *
   *     ["sci-fi", "thriller", "comedy"]
   *
   *     -> <Encode the arrays to vectors> ->
   *
   * | sci-fi | thriller | comedy | romance | drama |
   * |--------|----------|--------|---------|-------|
   * | 1      | 1        | 1      | 0       | 0     |
   *
   *     array vector: [1, 1, 1, 0, 0]
   */
  encode_categories_api_services_encoders_categories_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["VectorResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["CategoriesModel"];
      };
    };
  };
  /**
   * For example: a dictionary that represents a **person's characteristics visiting a store, field "person_characteristics"**:
   *
   *     {"height":180, "age":40, "weight":70}
   *
   *     -> <Encode the dictionary to vector> ->
   *
   * | height | age | weight | purchases | visits |
   * |--------|-----|--------|-----------|--------|
   * | 180    | 40  | 70     | 0         | 0      |
   *
   *     dictionary vector: [180, 40, 70, 0, 0]
   */
  encode_dictionary_api_services_encoders_dictionary_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["DictionaryModel"];
      };
    };
  };
  /** Encode text */
  encode_text_api_services_encoders_text_get: {
    parameters: {
      query: {
        /** Text to vectorise */
        text: string;
        /** The model url to use */
        model_url?: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Encode text */
  encode_text_api_services_encoders_multi_text_get: {
    parameters: {
      query: {
        /** Text to vectorise */
        text: string;
        /** The model url to use */
        model_url?: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Encode an image */
  encode_image_api_services_encoders_image_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["EncodeImageModel"];
      };
    };
  };
  /** Encode text to make searchable with images */
  encode_textimage_api_services_encoders_textimage_get: {
    parameters: {
      query: {
        /** Text to vectorise */
        text: string;
        /** The model url to use */
        model_url?: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /** Encode an image to make searchable with text */
  encode_imagetext_api_services_encoders_imagetext_get: {
    parameters: {
      query: {
        /** The image to encode (should be a URL to a model). If more data types are required please let us know. */
        image: string;
        /** The model url to use */
        model_url?: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /**
   * Receives a document and encode specified fields into vectors with provided model urls or model names.
   * e.g. [
   *     {"model_url" : "https://a_vector_model_url.com/encode_image_url", "body" : "url", "field": "thumbnail"},
   *     {"model_url" : "https://a_vector_model_url.com/encode_text", "body" : "text", "field": "short_description"},
   *     {"model_url" : "bert", "body" : "text", "field": "short_description", "alias":"bert"},
   * ]
   */
  retrieve_documents_api_services_encoders_encode_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["EncodeResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["EncodeModel"];
      };
    };
  };
  /**
   * Gets multiple documents and encodes specified fields into vectors with provided model urls or model names.
   * e.g. [
   *     {"model_url" : "https://a_vector_model_url.com/encode_image_url", "body" : "url", "field": "thumbnail"},
   *     {"model_url" : "https://a_vector_model_url.com/encode_text", "body" : "text", "field": "short_description"},
   *     {"model_url" : "bert", "body" : "text", "field": "short_description", "alias":"bert"},
   * ]
   */
  retrieve_documents_api_services_encoders_bulk_encode_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["BulkEncodeResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["BulkEncodeModel"];
      };
    };
  };
  /** Get frequency n-gram frequency counter from the wordcloud. */
  wordclouds_api_services_wordclouds_wordclouds_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": unknown;
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["WordCloudModel"];
      };
    };
  };
  /** Create a private deployable. */
  deployable_create_api_deployables_create_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["CreateDeployableResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["CreateDeployableBody"];
      };
    };
  };
  /**
   * Share a private deployable.
   *
   * The response should be {"status": "success"} or {"status": "failed"}
   */
  deployable_update_shareable_api_deployables__deployable_id__share_post: {
    parameters: {
      path: {
        deployable_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["UpdateDeployableResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  /**
   * Unshare a shared deployable, making it private.
   *
   * The response should be {"status": "success"} or {"status": "failed"}
   */
  deployable_update_private_api_deployables__deployable_id__private_post: {
    parameters: {
      path: {
        deployable_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["UpdateDeployableResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  deployable_update_api_deployables__deployable_id__update_post: {
    parameters: {
      path: {
        deployable_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["UpdateDeployableResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["UpdateDeployableBody"];
      };
    };
  };
  /** Get a deployable. */
  deployable_get_api_deployables__deployable_id__get_get: {
    parameters: {
      path: {
        deployable_id: string;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["GetDeployableResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
  deployable_delete_api_deployables_delete_post: {
    parameters: {
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["DeleteDeployableResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
    requestBody: {
      content: {
        "application/json": components["schemas"]["DeleteDeployableBody"];
      };
    };
  };
  /** List all deployables. */
  deployable_list_api_deployables_list_get: {
    parameters: {
      query: {
        /** Unique name of dataset, */
        dataset_id?: string;
        /** Page of the results */
        page?: number;
        /** Size of each page of results */
        page_size?: number;
      };
      header: {
        /** Authorization credentials. Header authorization should be in the form of **"project:api_key"** */
        Authorization: string;
      };
    };
    responses: {
      /** Successful Response */
      200: {
        content: {
          "application/json": components["schemas"]["ListDeployableResponse"];
        };
      };
      /** Validation Error */
      422: {
        content: {
          "application/json": components["schemas"]["HTTPValidationError"];
        };
      };
    };
  };
}

export interface external {}
